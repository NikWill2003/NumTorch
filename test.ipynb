{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple, Self, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import wandb\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "6c182ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular constants\n",
    "RNG = np.random.default_rng()\n",
    "DTYPE = 'float32' \n",
    "\n",
    "proc  = psutil.Process(os.getpid())\n",
    "\n",
    "# testing constants\n",
    "if DTYPE=='float64':\n",
    "    EPS, ATOL, RTOL = 1e-6, 1e-5, 1e-3\n",
    "else:\n",
    "    EPS, ATOL, RTOL = 1e-4, 1e-4, 1e-2\n",
    "K = 20\n",
    "\n",
    "dtype_eps = {'float16': 1e-4,\n",
    "             'float32': 1e-7,\n",
    "             'float64': 1e-15}[DTYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    def __init__(self, data, requires_grad=False, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data, dtype=DTYPE)\n",
    "        self.grad = np.zeros_like(data, dtype=DTYPE)\n",
    "        self.requires_grad = requires_grad\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int]:\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int: \n",
    "        return self.data.size\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = np.zeros_like(self.data, dtype=DTYPE)\n",
    "\n",
    "    def item(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "    def _unbroadcast(self, grad: np.ndarray) -> Self:\n",
    "        dims_to_remove = tuple(i for i in range(len(grad.shape) - len(self.shape))) \n",
    "        # remove prepended padding dimensions\n",
    "        grad = np.sum(grad, axis=dims_to_remove, keepdims=False) \n",
    "        dims_to_reduce = tuple(i for i, (d1,d2) in enumerate(zip(grad.shape, self.shape)) if d1!=d2)\n",
    "        # reduce broadcasted dimensions\n",
    "        return np.sum(grad, axis=dims_to_reduce, keepdims=True)\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self) -> None:\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            node._prev = set(())\n",
    "            node._backward = lambda : None\n",
    "\n",
    "    def __getitem__(self, indexes):\n",
    "        out = Tensor(self.data[indexes], self.requires_grad, (self), 'slice')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                pass\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "            \n",
    "    def __add__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data + rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '+')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self) -> Self:\n",
    "        out = Tensor(-self.data, self.requires_grad, (self,), 'neg')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, rhs) -> Self:\n",
    "        return self + (-rhs)\n",
    "\n",
    "    def __mul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data*rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), f'*')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * rhs.data)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, rhs) -> Self:\n",
    "        return self * (rhs**-1)\n",
    "    \n",
    "    # TODO add check for rhs, if epxponent if negative the gradient is undefined\n",
    "    def __pow__(self, rhs) -> Self: \n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        lhs_is_neg = self.data < 0\n",
    "        rhs_is_frac = ~np.isclose(rhs.data % 1, 0)\n",
    "        if np.any(lhs_is_neg & rhs_is_frac):\n",
    "            raise ValueError('cannot raise negative value to a decimal power')\n",
    "        \n",
    "        out = Tensor(self.data**rhs.data, self.requires_grad or rhs.requires_grad, (self,), f'**')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * ((rhs.data)*(self.data**(rhs.data-1))))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * (self.data ** rhs.data) * np.log(self.data))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''data shape: (da, ..., d2, d1, n, k) rhs shape: (ob, ..., o2, o1, k, m)\n",
    "       inputs are broadcast so that they have the same shape by expanding along\n",
    "       dimensions if possible, out shape: (tc, ..., t2, t1, n, m), where ti = max(di, oi)\n",
    "       if di or oi does not exist it is treated as 1, and c = max d, a\n",
    "       if self is 1d shape is prepended with a 1, for rhs it would be appended'''\n",
    "    def __matmul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data @ rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '@')\n",
    "\n",
    "        def _backward():\n",
    "            A, B, = self.data, rhs.data\n",
    "            g = out.grad\n",
    "            # broadcast 1d arrays to be 2d \n",
    "            A2 = A.reshape(1, -1) if len(A.shape) == 1 else A\n",
    "            B2 = B.reshape(-1, 1) if len(B.shape) == 1 else B\n",
    "            # extend g to have reduced dims\n",
    "            g = np.expand_dims(g, -1) if len(B.shape) == 1 else g\n",
    "            g = np.expand_dims(g, -2) if len(A.shape) == 1 else g\n",
    "            # transpose last 2 dimensions, as matmul treats tensors as batched matricies\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(g @ B2.swapaxes(-2, -1))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(A2.swapaxes(-2, -1) @ g)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Self:\n",
    "        out = Tensor((self.data > 0) * self.data, self.requires_grad, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self) -> Self:\n",
    "        if np.any(self.data < 0):\n",
    "            raise ValueError('cannot log negative values')\n",
    "        out = Tensor(np.log(self.data), self.requires_grad, (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self) -> Self:\n",
    "        out = Tensor(np.exp(self.data), self.requires_grad, (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=False) -> Self:\n",
    "        out = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims), self.requires_grad, (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                g = np.expand_dims(out.grad, axis) if (axis is not None and not keepdims) else out.grad\n",
    "                self.grad += g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis=None) -> Self:\n",
    "        out = Tensor(np.mean(self.data, axis=axis), self.requires_grad, (self,), 'mean')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                N =  self.size // out.size \n",
    "                g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "                self.grad += g / N\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def clamp(self, a_min=None, a_max=None):\n",
    "        out = Tensor(np.clip(self.data, a_min=a_min, a_max=a_max), self.requires_grad, (self,), 'clamp')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                mask = (self.data > a_min) if a_min is not None else np.ones_like(self.data)\n",
    "                mask = mask & (self.data < a_max) if a_max is not None else mask\n",
    "                self.grad += out.grad * mask\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rsub__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rmul__(self, lhs) -> Self:\n",
    "        return self * lhs\n",
    "    \n",
    "    def __rtruediv__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs / self\n",
    "    \n",
    "    def __rpow__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs ** self\n",
    "    \n",
    "    def __rmatmul__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs @ self\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape: tuple, bounds = (0,1), requires_grad=False) -> Self:\n",
    "        lower, upper = bounds\n",
    "        data = RNG.random(shape, dtype=DTYPE)*(upper-lower) + lower\n",
    "        return cls(data, requires_grad=requires_grad)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'tensor shape: {self.shape}, op:{self._op}'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "f977fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data, requires_grad=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def kaiming(cls, fan_in, shape):\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        weights = (RNG.standard_normal(shape, dtype='float64')*std).astype(dtype=DTYPE)\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        return cls(np.zeros(shape, dtype=DTYPE))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'parameter shape: {self.shape}, size: {self.size}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    @property\n",
    "    def modules(self) -> list[Self]:\n",
    "        modules: list[Self] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, Module):\n",
    "                modules.append(value)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "\n",
    "            elif isinstance(value, Iterable) and not isinstance(value, (str, bytes)):\n",
    "                for v in value:\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "                    \n",
    "        return modules\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> list[Parameter]:\n",
    "        immediate_params = [attr for attr in self.__dict__.values() \n",
    "                                    if isinstance(attr, Parameter)]\n",
    "        modules_params = [param for module in self.modules \n",
    "                                    for param in module.params]\n",
    "        return immediate_params + modules_params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def train(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def eval(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = False\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Affine(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.A = Parameter.kaiming(in_dim, (in_dim, out_dim))\n",
    "        self.b = Parameter.zeros((out_dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # x: (B, in), A : (in, out), B: out\n",
    "        return (x @ self.A) + self.b\n",
    "\n",
    "class Relu():\n",
    "    def __call__(self, x: Tensor):\n",
    "        return x.relu()\n",
    "    \n",
    "class SoftMax():\n",
    "    def __call__(self, x: Tensor):\n",
    "        # temporary as max is not an implemented op\n",
    "        x = x - np.max(x.data, axis=-1, keepdims=True) # for numerical stability \n",
    "        x = x.exp()\n",
    "        norm_c = x.sum(axis=-1, keepdims=True)\n",
    "        return x / norm_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "19219e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(array, num_c):\n",
    "    one_hot = np.zeros(shape=(array.size, num_c))\n",
    "    for idx, i in enumerate(array):\n",
    "        one_hot[idx, i] = 1\n",
    "    return one_hot\n",
    "\n",
    "y = np.arange(5)\n",
    "print(y)\n",
    "print(one_hot_encode(y, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(array, num_c):\n",
    "    one_hot = np.zeros(shape=(array.size, num_c))\n",
    "    for idx, i in enumerate(array):\n",
    "        one_hot[idx, i] = 1\n",
    "    return one_hot\n",
    "\n",
    "class SoftMaxCrossEntropy():\n",
    "    def __call__(self, z: Tensor, y) -> Tensor:\n",
    "        '''logits z, shape (B, C), true integer lables y, shape (B)'''\n",
    "        # TODO change from manual one hot encoding when getitem is implemented in tensor\n",
    "        y = one_hot_encode(y, z.shape[-1]) #shape (B, C)\n",
    "        z = z - np.max(z.data, axis=-1, keepdims=True) # for numerical stability \n",
    "        loss = (-(z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "        return loss\n",
    "\n",
    "class CrossEntropy():\n",
    "    def __call__(self, q: Tensor, y) -> Tensor:\n",
    "        '''pred q, shape (B, C), true integer lables y, shape (B)'''\n",
    "        # TODO change from manual one hot encoding when getitem is implemented in tensor\n",
    "        y = Tensor(one_hot_encode(y, q.shape[-1])) #shape (B, C)\n",
    "        loss = -(y * (q+dtype_eps).log()).sum(axis=-1).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class MeanSquaredError():\n",
    "    def __call__(self, q: Tensor, y) -> Tensor:\n",
    "        '''pred q, shape (B, C), true values y, shape (B, C)'''\n",
    "        loss = ((q - y) ** 2).sum(axis=-1).mean()\n",
    "        return loss\n",
    "\n",
    "class optimiser():\n",
    "    def __init__(self, params: list[Parameter], lr: float=0.005):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def train(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def eval(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "class SGD(optimiser):\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for param in self.params:\n",
    "            if not param.requires_grad:\n",
    "                continue \n",
    "            param.data += -self.lr * param.grad\n",
    "\n",
    "class Adam(optimiser):\n",
    "    def __init__(self, params: list[Parameter], lr: float=0.005, \n",
    "                 betas: Tuple[float, float]=(0.9, 0.999), eps: float=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.b1 , self.b2 = betas\n",
    "        self.eps = eps\n",
    "        self.time_step = 0\n",
    "        self.m = [np.zeros_like(param.data, dtype=DTYPE) for param in params]\n",
    "        self.v = [np.zeros_like(param.data, dtype=DTYPE) for param in params]\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        self.time_step += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            if not p.requires_grad:\n",
    "                continue \n",
    "\n",
    "            g = p.grad\n",
    "            self.m[i] = self.b1*self.m[i] + (1-self.b1)*g\n",
    "            self.v[i] = self.b2*self.v[i] + (1-self.b2)*(g**2)\n",
    "            m_hat = self.m[i]/(1-self.b1**self.time_step)\n",
    "            v_hat = self.v[i]/(1-self.b2**self.time_step)\n",
    "\n",
    "            p.data += -self.lr * m_hat / (v_hat ** 0.5 + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d3a259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimiser, loss, train_loader, test_loader, logger, wandb_run = None):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss = loss\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epoch = 1\n",
    "        self.logger = logger\n",
    "        self.wandb_run = wandb_run\n",
    "\n",
    "    def train_epoch():\n",
    "        pass\n",
    "\n",
    "    def validate():\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def log_metrics():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9929b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, input_data, true_data, batch_size, shuffle=False, rng: np.random.Generator=RNG):\n",
    "        assert input_data.shape[0] == true_data.shape[0], 'must have the same number of inputs and true outputs'\n",
    "        self.X = input_data\n",
    "        self.y = true_data\n",
    "        self.N = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = rng\n",
    "\n",
    "    def __iter__(self):\n",
    "        X, y = self.X, self.y\n",
    "        if self.shuffle:\n",
    "            permutation = self.rng.permutation(X.shape[0])\n",
    "            X = X[permutation]\n",
    "            y = y[permutation]\n",
    "        splits = np.arange(self.N, X.shape[0], self.N)\n",
    "        X = np.split(X, splits, axis=0)\n",
    "        y = np.split(y, splits, axis=0)\n",
    "        return zip(X, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # samples/batch size rounded up\n",
    "        return ceil(self.X.shape[0]/self.N)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4c6539cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request, numpy as np\n",
    "# import os\n",
    "\n",
    "# os.makedirs('datasets')\n",
    "\n",
    "# url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "local_path = r\"datasets\\\\mnist.npz\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, local_path)   # â‡¦ makes a real file\n",
    "data = np.load(local_path)\n",
    "\n",
    "# im = X_train[0:3]\n",
    "# print(type(im))\n",
    "# plt.imshow(im, cmap='grey')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "031abe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "[1 9 1 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAF/dJREFUeJzt3X9o1Pcdx/HXqfGrdZcbmSZ3V+MRitJSRfDH1Kz1R8HDgE7rxmwLI/4j7fwBLi0yJ8Nsf5giVDrIalkZTlnd/GPqhEo1Q5M4XIYVS8WKpBibDD2Cwd7FaC9YP/tDPHYmxly8yzuXez7gC973h/f2u+989uv9iM855wQAgIEx1gMAAAoXEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGbGWQ/wqPv37+v69evy+/3y+XzW4wAAMuScU3d3t8LhsMaMGfheZ8RF6Pr16yovL7ceAwDwlDo6OjR16tQB9xlx/xzn9/utRwAAZMFg/j7PWYQ++OADVVRUaMKECZo7d67OnDkzqOP4JzgAGB0G8/d5TiJ06NAhbd26VTt27NCFCxf08ssvq6qqSu3t7bl4OgBAnvLl4lu0FyxYoDlz5mjv3r2pdS+88ILWrFmjurq6AY9NJBIKBALZHgkAMMzi8biKi4sH3Cfrd0K9vb06f/68otFo2vpoNKqzZ8/22T+ZTCqRSKQtAIDCkPUI3bx5U999953KysrS1peVlSkWi/XZv66uToFAILXwzjgAKBw5e2PCoy9IOef6fZFq+/btisfjqaWjoyNXIwEARpisf05o8uTJGjt2bJ+7ns7Ozj53R5LkeZ48z8v2GACAPJD1O6Hx48dr7ty5amhoSFvf0NCgysrKbD8dACCP5eQbE2pqavTzn/9c8+bN06JFi/THP/5R7e3teuutt3LxdACAPJWTCK1bt05dXV363e9+pxs3bmjmzJk6fvy4IpFILp4OAJCncvI5oafB54QAYHQw+ZwQAACDRYQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJgZZz0AMJIUFRVlfExlZWXGx+zatSvjY370ox9lfAww0nEnBAAwQ4QAAGayHqHa2lr5fL60JRgMZvtpAACjQE5eE3rxxRf1z3/+M/V47NixuXgaAECey0mExo0bx90PAOCJcvKaUGtrq8LhsCoqKvTaa6/p6tWrj903mUwqkUikLQCAwpD1CC1YsEAHDhzQiRMn9NFHHykWi6myslJdXV397l9XV6dAIJBaysvLsz0SAGCE8jnnXC6foKenR88995y2bdummpqaPtuTyaSSyWTqcSKRIEQww+eEgOyJx+MqLi4ecJ+cf1h10qRJmjVrllpbW/vd7nmePM/L9RgAgBEo558TSiaTunz5skKhUK6fCgCQZ7IeoXfeeUdNTU1qa2vTf/7zH/30pz9VIpFQdXV1tp8KAJDnsv7Pcf/973/1+uuv6+bNm5oyZYoWLlyolpYWRSKRbD8VACDP5fyNCZlKJBIKBALWY6BATZ48OeNjOjs7Mz4mFotlfMycOXOG5XmAbBnMGxP47jgAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwEzOf6gdgL6CweCwHMMXmGKk404IAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZvgWbcCAz+ezHgEYEbgTAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDM8AWmgAHnXMbHTJgwIQeTALa4EwIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzPAFpkCemDdvXsbHtLS05GASIHu4EwIAmCFCAAAzGUeoublZq1atUjgcls/n09GjR9O2O+dUW1urcDisiRMnaunSpbp06VK25gUAjCIZR6inp0ezZ89WfX19v9t3796tPXv2qL6+XufOnVMwGNTy5cvV3d391MMCAEaXjN+YUFVVpaqqqn63Oef0/vvva8eOHVq7dq0kaf/+/SorK9PBgwf15ptvPt20AIBRJauvCbW1tSkWiykajabWeZ6nJUuW6OzZs/0ek0wmlUgk0hYAQGHIaoRisZgkqaysLG19WVlZatuj6urqFAgEUkt5eXk2RwIAjGA5eXecz+dLe+yc67Puoe3btysej6eWjo6OXIwEABiBsvph1WAwKOnBHVEoFEqt7+zs7HN39JDnefI8L5tjAADyRFbvhCoqKhQMBtXQ0JBa19vbq6amJlVWVmbzqQAAo0DGd0K3b9/WV199lXrc1tamzz//XCUlJZo2bZq2bt2qXbt2afr06Zo+fbp27dqlZ555Rm+88UZWBwcA5L+MI/TZZ59p2bJlqcc1NTWSpOrqav35z3/Wtm3bdPfuXW3cuFG3bt3SggULdPLkSfn9/uxNDQAYFXzOOWc9xP9LJBIKBALWY6BAff/738/4mLa2toyPGco1/vvf/z7jY375y19mfAyQLfF4XMXFxQPuw3fHAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwExWf7IqkO+++eabjI85c+ZMxsesXLky42OA0Yg7IQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmXHWAwAYnB/84AfWIwBZx50QAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGLzAF8sSPf/xj6xGArONOCABghggBAMxkHKHm5matWrVK4XBYPp9PR48eTdu+fv16+Xy+tGXhwoXZmhcAMIpkHKGenh7Nnj1b9fX1j91nxYoVunHjRmo5fvz4Uw0JABidMn5jQlVVlaqqqgbcx/M8BYPBIQ8FACgMOXlNqLGxUaWlpZoxY4Y2bNigzs7Ox+6bTCaVSCTSFgBAYch6hKqqqvTxxx/r1KlTeu+993Tu3Dm98sorSiaT/e5fV1enQCCQWsrLy7M9EgBghMr654TWrVuX+vXMmTM1b948RSIRffLJJ1q7dm2f/bdv366amprU40QiQYgAoEDk/MOqoVBIkUhEra2t/W73PE+e5+V6DADACJTzzwl1dXWpo6NDoVAo108FAMgzGd8J3b59W1999VXqcVtbmz7//HOVlJSopKREtbW1+slPfqJQKKRr167p17/+tSZPnqxXX301q4MDAPJfxhH67LPPtGzZstTjh6/nVFdXa+/evbp48aIOHDigb775RqFQSMuWLdOhQ4fk9/uzNzUAYFTIOEJLly6Vc+6x20+cOPFUAwH55vTp0xkfs3LlyhxMAuQfvjsOAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnL+k1WB0a69vX1YnqeoqCjjYyKRyJCe6+uvvx7ScUCmuBMCAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMzwBabAU7p3796wPI/P58v4GM/zcjAJkD3cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnzOOWc9xP9LJBIKBALWYwA59eWXX2Z8zPPPP5/xMR9++GHGx0jSxo0bh3Qc8P/i8biKi4sH3Ic7IQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADAzDjrAYBCdPLkyYyPefbZZzM+pqamJuNjgOHEnRAAwAwRAgCYyShCdXV1mj9/vvx+v0pLS7VmzRpduXIlbR/nnGpraxUOhzVx4kQtXbpUly5dyurQAIDRIaMINTU1adOmTWppaVFDQ4Pu3bunaDSqnp6e1D67d+/Wnj17VF9fr3PnzikYDGr58uXq7u7O+vAAgPyW0RsTPv3007TH+/btU2lpqc6fP6/FixfLOaf3339fO3bs0Nq1ayVJ+/fvV1lZmQ4ePKg333wze5MDAPLeU70mFI/HJUklJSWSpLa2NsViMUWj0dQ+nudpyZIlOnv2bL+/RzKZVCKRSFsAAIVhyBFyzqmmpkYvvfSSZs6cKUmKxWKSpLKysrR9y8rKUtseVVdXp0AgkFrKy8uHOhIAIM8MOUKbN2/WF198ob/+9a99tvl8vrTHzrk+6x7avn274vF4auno6BjqSACAPDOkD6tu2bJFx44dU3Nzs6ZOnZpaHwwGJT24IwqFQqn1nZ2dfe6OHvI8T57nDWUMAECey+hOyDmnzZs36/Dhwzp16pQqKirStldUVCgYDKqhoSG1rre3V01NTaqsrMzOxACAUSOjO6FNmzbp4MGD+sc//iG/3596nScQCGjixIny+XzaunWrdu3apenTp2v69OnatWuXnnnmGb3xxhs5+QMAAPJXRhHau3evJGnp0qVp6/ft26f169dLkrZt26a7d+9q48aNunXrlhYsWKCTJ0/K7/dnZWAAwOiRUYScc0/cx+fzqba2VrW1tUOdCUA/BvP/v0f19vbmYBIge/juOACAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJgZ0k9WBTD8iouLMz5m9erVQ3quI0eODOk4IFPcCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZvgCU8DAz372s4yPSSaTGR9z+fLljI8BhhN3QgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGb7AFDDQ3Nyc8TEvvPBCxsfcvXs342OA4cSdEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgxuecc9ZD/L9EIqFAIGA9BgDgKcXjcRUXFw+4D3dCAAAzRAgAYCajCNXV1Wn+/Pny+/0qLS3VmjVrdOXKlbR91q9fL5/Pl7YsXLgwq0MDAEaHjCLU1NSkTZs2qaWlRQ0NDbp3756i0ah6enrS9luxYoVu3LiRWo4fP57VoQEAo0NGP1n1008/TXu8b98+lZaW6vz581q8eHFqved5CgaD2ZkQADBqPdVrQvF4XJJUUlKStr6xsVGlpaWaMWOGNmzYoM7Ozsf+HslkUolEIm0BABSGIb9F2zmn1atX69atWzpz5kxq/aFDh/S9731PkUhEbW1t+s1vfqN79+7p/Pnz8jyvz+9TW1ur3/72t0P/EwAARqTBvEVbbog2btzoIpGI6+joGHC/69evu6KiIvf3v/+93+3ffvuti8fjqaWjo8NJYmFhYWHJ8yUejz+xJRm9JvTQli1bdOzYMTU3N2vq1KkD7hsKhRSJRNTa2trvds/z+r1DAgCMfhlFyDmnLVu26MiRI2psbFRFRcUTj+nq6lJHR4dCodCQhwQAjE4ZvTFh06ZN+stf/qKDBw/K7/crFospFovp7t27kqTbt2/rnXfe0b///W9du3ZNjY2NWrVqlSZPnqxXX301J38AAEAey+R1ID3m3/327dvnnHPuzp07LhqNuilTpriioiI3bdo0V11d7drb2wf9HPF43PzfMVlYWFhYnn4ZzGtCfIEpACAn+AJTAMCIRoQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwM+Ii5JyzHgEAkAWD+ft8xEWou7vbegQAQBYM5u9znxthtx7379/X9evX5ff75fP50rYlEgmVl5ero6NDxcXFRhPa4zw8wHl4gPPwAOfhgZFwHpxz6u7uVjgc1pgxA9/rjBummQZtzJgxmjp16oD7FBcXF/RF9hDn4QHOwwOchwc4Dw9Yn4dAIDCo/UbcP8cBAAoHEQIAmMmrCHmep507d8rzPOtRTHEeHuA8PMB5eIDz8EC+nYcR98YEAEDhyKs7IQDA6EKEAABmiBAAwAwRAgCYyasIffDBB6qoqNCECRM0d+5cnTlzxnqkYVVbWyufz5e2BINB67Fyrrm5WatWrVI4HJbP59PRo0fTtjvnVFtbq3A4rIkTJ2rp0qW6dOmSzbA59KTzsH79+j7Xx8KFC22GzZG6ujrNnz9ffr9fpaWlWrNmja5cuZK2TyFcD4M5D/lyPeRNhA4dOqStW7dqx44dunDhgl5++WVVVVWpvb3derRh9eKLL+rGjRup5eLFi9Yj5VxPT49mz56t+vr6frfv3r1be/bsUX19vc6dO6dgMKjly5ePuu8hfNJ5kKQVK1akXR/Hjx8fxglzr6mpSZs2bVJLS4saGhp07949RaNR9fT0pPYphOthMOdBypPrweWJH/7wh+6tt95KW/f888+7X/3qV0YTDb+dO3e62bNnW49hSpI7cuRI6vH9+/ddMBh07777bmrdt99+6wKBgPvwww8NJhwej54H55yrrq52q1evNpnHSmdnp5PkmpqanHOFez08eh6cy5/rIS/uhHp7e3X+/HlFo9G09dFoVGfPnjWaykZra6vC4bAqKir02muv6erVq9YjmWpra1MsFku7NjzP05IlSwru2pCkxsZGlZaWasaMGdqwYYM6OzutR8qpeDwuSSopKZFUuNfDo+fhoXy4HvIiQjdv3tR3332nsrKytPVlZWWKxWJGUw2/BQsW6MCBAzpx4oQ++ugjxWIxVVZWqqury3o0Mw//9y/0a0OSqqqq9PHHH+vUqVN67733dO7cOb3yyitKJpPWo+WEc041NTV66aWXNHPmTEmFeT30dx6k/LkeRty3aA/k0R/t4Jzrs240q6qqSv161qxZWrRokZ577jnt379fNTU1hpPZK/RrQ5LWrVuX+vXMmTM1b948RSIRffLJJ1q7dq3hZLmxefNmffHFF/rXv/7VZ1shXQ+POw/5cj3kxZ3Q5MmTNXbs2D7/JdPZ2dnnv3gKyaRJkzRr1iy1trZaj2Lm4bsDuTb6CoVCikQio/L62LJli44dO6bTp0+n/eiXQrseHnce+jNSr4e8iND48eM1d+5cNTQ0pK1vaGhQZWWl0VT2ksmkLl++rFAoZD2KmYqKCgWDwbRro7e3V01NTQV9bUhSV1eXOjo6RtX14ZzT5s2bdfjwYZ06dUoVFRVp2wvlenjSeejPiL0eDN8UkZG//e1vrqioyP3pT39yX375pdu6daubNGmSu3btmvVow+btt992jY2N7urVq66lpcWtXLnS+f3+UX8Ouru73YULF9yFCxecJLdnzx534cIF9/XXXzvnnHv33XddIBBwhw8fdhcvXnSvv/66C4VCLpFIGE+eXQOdh+7ubvf222+7s2fPura2Nnf69Gm3aNEi9+yzz46q8/CLX/zCBQIB19jY6G7cuJFa7ty5k9qnEK6HJ52HfLoe8iZCzjn3hz/8wUUiETd+/Hg3Z86ctLcjFoJ169a5UCjkioqKXDgcdmvXrnWXLl2yHivnTp8+7ST1Waqrq51zD96Wu3PnThcMBp3neW7x4sXu4sWLtkPnwEDn4c6dOy4ajbopU6a4oqIiN23aNFddXe3a29utx86q/v78kty+fftS+xTC9fCk85BP1wM/ygEAYCYvXhMCAIxORAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZ/wEy+VoZ1HMTIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 4 5]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 5 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, y_train = data[\"x_train\"][0:12].reshape((-1,784)) / 255, data[\"y_train\"][0:12]\n",
    "print(y_train.shape)\n",
    "train_loader = DataLoader(X_train, y_train, 4, shuffle=True)\n",
    "for X, y in train_loader:\n",
    "    print(y)\n",
    "    im = X[0].reshape((28,28))\n",
    "    print(type(im))\n",
    "    plt.imshow(im, cmap='grey')\n",
    "    plt.show()\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "725b5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(array, num_c):\n",
    "    one_hot = np.zeros(shape=(array.size, num_c))\n",
    "    for idx, i in enumerate(array):\n",
    "        one_hot[idx, i] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "58fc738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (784, 100), size: 78400,\n",
       " parameter shape: (100,), size: 100,\n",
       " parameter shape: (100, 200), size: 20000,\n",
       " parameter shape: (200,), size: 200,\n",
       " parameter shape: (200, 10), size: 2000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Sequential([Affine(784, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10), SoftMax()])\n",
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "0202b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAverageMeter():\n",
    "    def __init__(self):\n",
    "        self._metrics = {}\n",
    "        self._counts = {}\n",
    "    \n",
    "    def update(self, metric, mean, n=1):\n",
    "        if metric in self._metrics.keys():\n",
    "            k = self._counts[metric]\n",
    "            self._metrics[metric] += n*(mean - self._metrics[metric])/(k+n)\n",
    "            self._counts[metric] += n\n",
    "        else:\n",
    "            self._metrics[metric] = mean\n",
    "            self._counts[metric] = n\n",
    "    \n",
    "    def get_metric(self, metric):\n",
    "        if metric in self._metrics:\n",
    "            return  self._metrics[metric]\n",
    "        raise KeyError(f'{metric} not found')\n",
    "    \n",
    "    def dump_metrics(self):\n",
    "        return self._metrics\n",
    "\n",
    "    def reset(self, metric=None):\n",
    "        if metric is None:\n",
    "            self._metrics = {}\n",
    "            self._counts = {}\n",
    "        else:\n",
    "            del self._metrics[metric]\n",
    "            del self._counts[metric]\n",
    "\n",
    "    def get_log_str(self, metrics=None):\n",
    "        log_str = ''\n",
    "        metrics = self._metrics.keys() if metrics is None else metrics\n",
    "        for metric in metrics:\n",
    "            if metric not in self._metrics:\n",
    "                continue\n",
    "            log_str += f'{metric} : {self._metrics[metric]:.4f} '\n",
    "        return log_str\n",
    "\n",
    "    def __getitem__(self, key):  \n",
    "        return self.get_metric(key)\n",
    "    \n",
    "    def __contains__(self, key): \n",
    "        return key in self._metrics\n",
    "    \n",
    "    def __iter__(self):          \n",
    "        return iter(self._metrics)\n",
    "    \n",
    "    def items(self):            \n",
    "        return self._metrics.items()\n",
    "    \n",
    "    def __len__(self):           \n",
    "        return len(self._metrics)\n",
    "    \n",
    "    def __repr__(self):          \n",
    "        return f\"MultiAverageMeter({self._metrics})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1bc98108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wandb_run(wandb_config:dict, api_key:str|None=None):\n",
    "    if api_key is None:\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "    assert api_key is not None, 'api key required'\n",
    "\n",
    "    wandb.login(key=api_key)\n",
    "    return wandb.init(**wandb_config)\n",
    "\n",
    "\n",
    "class WandBLogger():\n",
    "    def __init__(self, meter:MultiAverageMeter, wandb_config:dict, api_key:str|None=None):\n",
    "        self._meter = meter\n",
    "        self._wandb_run = get_wandb_run(wandb_config, api_key)\n",
    "        self._mode = 'train'\n",
    "\n",
    "    def eval(self):\n",
    "        self._mode = 'eval'\n",
    "\n",
    "    def train(self):\n",
    "        self._mode = 'train'\n",
    "\n",
    "    def test(self):\n",
    "        self._mode = 'test'\n",
    "\n",
    "    def log_epoch(self, epoch):\n",
    "        metrics = {}\n",
    "        for name, val in self._meter.items():\n",
    "            metrics[name + f'/{self._mode}'] = val\n",
    "        metrics = metrics | self.get_system_metrics()\n",
    "        self._wandb_run.log(metrics, step=epoch)\n",
    "\n",
    "    def get_system_metrics(self):\n",
    "        metrics = {\n",
    "            'sys/ram_gb' : proc.memory_info().rss / 1_073_741_824\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    def finish(self):\n",
    "        self._wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "814187de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {'project': 'torch from scratch testing',\n",
    "                'name': 'mnist_test_1',\n",
    "                'config': {'optimiser':'adam', 'lr':0.05},\n",
    "                'group': 'mnist tests',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "bff5d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_step(meter:MultiAverageMeter, train, nn, loader, loss_fn, optimiser):\n",
    "    start = time.time()\n",
    "    if train:\n",
    "        nn.train()\n",
    "    else:\n",
    "        nn.eval()\n",
    "\n",
    "    for X, y in loader:\n",
    "\n",
    "        nn.zero_grad()\n",
    "        out = nn(Tensor(X))\n",
    "        loss = loss_fn(out, y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        preds = np.argmax(out.item(), axis=-1)\n",
    "        acc = np.sum(preds == y) / preds.size \n",
    "        \n",
    "        meter.update('CE', loss.item(), y.shape[0])\n",
    "        meter.update('accuracy', acc, y.shape[0])\n",
    "    end = time.time()\n",
    "    meter.update('speed/epoch_sec', end - start)\n",
    "    meter.update('speed/samples_per_sec', len(loader) / (end - start))\n",
    "    return meter.get_log_str()\n",
    "\n",
    "def train_nn(epochs):\n",
    "\n",
    "    meter = MultiAverageMeter()\n",
    "    wandb_logger = WandBLogger(meter, wandb_config)\n",
    "\n",
    "    X_train, y_train = data[\"x_train\"].reshape((-1,784)) / 255, data[\"y_train\"]\n",
    "    X_test, y_test = data[\"x_test\"].reshape((-1,784)) / 255, data[\"y_test\"]\n",
    "\n",
    "    train_loader = DataLoader(X_train, y_train, 256, shuffle=True)\n",
    "    test_loader = DataLoader(X_test, y_test, 256, shuffle=False)\n",
    "\n",
    "    nn = Sequential([Affine(784, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "    loss_fn = SoftMaxCrossEntropy()\n",
    "    # optimiser = Adam(nn.params)\n",
    "    optimiser = SGD(nn.params, lr=0.05)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f'epoch: {t}')\n",
    "        wandb_logger.train()\n",
    "        meter.reset()\n",
    "        log = train_test_step(meter, True, nn, train_loader, loss_fn, optimiser)\n",
    "        print('train: ' + log)\n",
    "        wandb_logger.log_epoch(t)\n",
    "\n",
    "        wandb_logger.eval()\n",
    "        meter.reset()\n",
    "        log = train_test_step(meter, False, nn, test_loader, loss_fn, optimiser)\n",
    "        print('test: ' + log)\n",
    "        wandb_logger.log_epoch(t)\n",
    "\n",
    "    wandb_logger.finish()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "2335e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\nikiw\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nikiw\\dl_projects\\dqn_from_scratch\\wandb\\run-20250722_153949-99yotdm1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/99yotdm1' target=\"_blank\">mnist_test_1</a></strong> to <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/99yotdm1' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/99yotdm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train: CE : 0.6605 accuracy : 0.8229 speed/epoch_sec : 0.4172 speed/samples_per_sec : 563.3391 \n",
      "test: CE : 0.3445 accuracy : 0.9037 speed/epoch_sec : 0.0331 speed/samples_per_sec : 1206.9853 \n",
      "epoch: 1\n",
      "train: CE : 0.3117 accuracy : 0.9101 speed/epoch_sec : 0.4131 speed/samples_per_sec : 568.8045 \n",
      "test: CE : 0.2751 accuracy : 0.9216 speed/epoch_sec : 0.0306 speed/samples_per_sec : 1306.5045 \n",
      "epoch: 2\n",
      "train: CE : 0.2595 accuracy : 0.9252 speed/epoch_sec : 0.4199 speed/samples_per_sec : 559.6600 \n",
      "test: CE : 0.2494 accuracy : 0.9245 speed/epoch_sec : 0.0457 speed/samples_per_sec : 875.7153 \n",
      "epoch: 3\n",
      "train: CE : 0.2281 accuracy : 0.9342 speed/epoch_sec : 0.4154 speed/samples_per_sec : 565.7243 \n",
      "test: CE : 0.2223 accuracy : 0.9343 speed/epoch_sec : 0.0268 speed/samples_per_sec : 1492.3561 \n",
      "epoch: 4\n",
      "train: CE : 0.2038 accuracy : 0.9418 speed/epoch_sec : 0.4205 speed/samples_per_sec : 558.9196 \n",
      "test: CE : 0.1936 accuracy : 0.9436 speed/epoch_sec : 0.0357 speed/samples_per_sec : 1121.1045 \n",
      "epoch: 5\n",
      "train: CE : 0.1849 accuracy : 0.9477 speed/epoch_sec : 0.4436 speed/samples_per_sec : 529.8093 \n",
      "test: CE : 0.1788 accuracy : 0.9478 speed/epoch_sec : 0.0300 speed/samples_per_sec : 1332.3287 \n",
      "epoch: 6\n",
      "train: CE : 0.1705 accuracy : 0.9511 speed/epoch_sec : 0.4380 speed/samples_per_sec : 536.5312 \n",
      "test: CE : 0.1694 accuracy : 0.9496 speed/epoch_sec : 0.0360 speed/samples_per_sec : 1112.0903 \n",
      "epoch: 7\n",
      "train: CE : 0.1579 accuracy : 0.9557 speed/epoch_sec : 0.4432 speed/samples_per_sec : 530.2442 \n",
      "test: CE : 0.1544 accuracy : 0.9534 speed/epoch_sec : 0.0361 speed/samples_per_sec : 1108.8782 \n",
      "epoch: 8\n",
      "train: CE : 0.1468 accuracy : 0.9579 speed/epoch_sec : 0.4327 speed/samples_per_sec : 543.0977 \n",
      "test: CE : 0.1478 accuracy : 0.9565 speed/epoch_sec : 0.0321 speed/samples_per_sec : 1245.5708 \n",
      "epoch: 9\n",
      "train: CE : 0.1373 accuracy : 0.9609 speed/epoch_sec : 0.4634 speed/samples_per_sec : 507.1313 \n",
      "test: CE : 0.1467 accuracy : 0.9563 speed/epoch_sec : 0.0387 speed/samples_per_sec : 1033.0226 \n",
      "epoch: 10\n",
      "train: CE : 0.1296 accuracy : 0.9635 speed/epoch_sec : 0.4680 speed/samples_per_sec : 502.1284 \n",
      "test: CE : 0.1396 accuracy : 0.9581 speed/epoch_sec : 0.0423 speed/samples_per_sec : 945.8454 \n",
      "epoch: 11\n",
      "train: CE : 0.1223 accuracy : 0.9657 speed/epoch_sec : 0.4747 speed/samples_per_sec : 495.0928 \n",
      "test: CE : 0.1283 accuracy : 0.9604 speed/epoch_sec : 0.0379 speed/samples_per_sec : 1055.3168 \n",
      "epoch: 12\n",
      "train: CE : 0.1156 accuracy : 0.9672 speed/epoch_sec : 0.4792 speed/samples_per_sec : 490.3966 \n",
      "test: CE : 0.1238 accuracy : 0.9627 speed/epoch_sec : 0.0341 speed/samples_per_sec : 1172.0025 \n",
      "epoch: 13\n",
      "train: CE : 0.1098 accuracy : 0.9689 speed/epoch_sec : 0.4784 speed/samples_per_sec : 491.2149 \n",
      "test: CE : 0.1190 accuracy : 0.9635 speed/epoch_sec : 0.0415 speed/samples_per_sec : 964.2577 \n",
      "epoch: 14\n",
      "train: CE : 0.1045 accuracy : 0.9704 speed/epoch_sec : 0.5002 speed/samples_per_sec : 469.7776 \n",
      "test: CE : 0.1174 accuracy : 0.9645 speed/epoch_sec : 0.0416 speed/samples_per_sec : 961.5387 \n",
      "epoch: 15\n",
      "train: CE : 0.0993 accuracy : 0.9723 speed/epoch_sec : 0.4759 speed/samples_per_sec : 493.7496 \n",
      "test: CE : 0.1121 accuracy : 0.9659 speed/epoch_sec : 0.0386 speed/samples_per_sec : 1034.9726 \n",
      "epoch: 16\n",
      "train: CE : 0.0951 accuracy : 0.9727 speed/epoch_sec : 0.4708 speed/samples_per_sec : 499.1118 \n",
      "test: CE : 0.1097 accuracy : 0.9658 speed/epoch_sec : 0.0339 speed/samples_per_sec : 1179.8406 \n",
      "epoch: 17\n",
      "train: CE : 0.0908 accuracy : 0.9742 speed/epoch_sec : 0.4836 speed/samples_per_sec : 485.9382 \n",
      "test: CE : 0.1087 accuracy : 0.9667 speed/epoch_sec : 0.0376 speed/samples_per_sec : 1062.8514 \n",
      "epoch: 18\n",
      "train: CE : 0.0868 accuracy : 0.9757 speed/epoch_sec : 0.4955 speed/samples_per_sec : 474.2839 \n",
      "test: CE : 0.1048 accuracy : 0.9692 speed/epoch_sec : 0.0359 speed/samples_per_sec : 1114.7504 \n",
      "epoch: 19\n",
      "train: CE : 0.0830 accuracy : 0.9766 speed/epoch_sec : 0.5166 speed/samples_per_sec : 454.8891 \n",
      "test: CE : 0.1073 accuracy : 0.9661 speed/epoch_sec : 0.0371 speed/samples_per_sec : 1077.7286 \n",
      "epoch: 20\n",
      "train: CE : 0.0800 accuracy : 0.9778 speed/epoch_sec : 0.4842 speed/samples_per_sec : 485.3543 \n",
      "test: CE : 0.0993 accuracy : 0.9697 speed/epoch_sec : 0.0331 speed/samples_per_sec : 1207.8629 \n",
      "epoch: 21\n",
      "train: CE : 0.0764 accuracy : 0.9781 speed/epoch_sec : 0.5059 speed/samples_per_sec : 464.5041 \n",
      "test: CE : 0.1035 accuracy : 0.9687 speed/epoch_sec : 0.0392 speed/samples_per_sec : 1020.5492 \n",
      "epoch: 22\n",
      "train: CE : 0.0736 accuracy : 0.9792 speed/epoch_sec : 0.4895 speed/samples_per_sec : 480.0679 \n",
      "test: CE : 0.0988 accuracy : 0.9705 speed/epoch_sec : 0.0389 speed/samples_per_sec : 1029.2834 \n",
      "epoch: 23\n",
      "train: CE : 0.0709 accuracy : 0.9800 speed/epoch_sec : 0.4943 speed/samples_per_sec : 475.4410 \n",
      "test: CE : 0.0936 accuracy : 0.9713 speed/epoch_sec : 0.0378 speed/samples_per_sec : 1058.5996 \n",
      "epoch: 24\n",
      "train: CE : 0.0679 accuracy : 0.9809 speed/epoch_sec : 0.5141 speed/samples_per_sec : 457.0810 \n",
      "test: CE : 0.0907 accuracy : 0.9726 speed/epoch_sec : 0.0415 speed/samples_per_sec : 963.9807 \n",
      "epoch: 25\n",
      "train: CE : 0.0656 accuracy : 0.9819 speed/epoch_sec : 0.5109 speed/samples_per_sec : 459.9511 \n",
      "test: CE : 0.0959 accuracy : 0.9697 speed/epoch_sec : 0.0395 speed/samples_per_sec : 1012.5299 \n",
      "epoch: 26\n",
      "train: CE : 0.0631 accuracy : 0.9819 speed/epoch_sec : 0.5127 speed/samples_per_sec : 458.3442 \n",
      "test: CE : 0.0921 accuracy : 0.9700 speed/epoch_sec : 0.0400 speed/samples_per_sec : 999.1315 \n",
      "epoch: 27\n",
      "train: CE : 0.0611 accuracy : 0.9830 speed/epoch_sec : 0.5214 speed/samples_per_sec : 450.7296 \n",
      "test: CE : 0.0944 accuracy : 0.9723 speed/epoch_sec : 0.0363 speed/samples_per_sec : 1102.6543 \n",
      "epoch: 28\n",
      "train: CE : 0.0587 accuracy : 0.9837 speed/epoch_sec : 0.5206 speed/samples_per_sec : 451.3703 \n",
      "test: CE : 0.0866 accuracy : 0.9724 speed/epoch_sec : 0.0409 speed/samples_per_sec : 978.6913 \n",
      "epoch: 29\n",
      "train: CE : 0.0563 accuracy : 0.9847 speed/epoch_sec : 0.5116 speed/samples_per_sec : 459.3640 \n",
      "test: CE : 0.0853 accuracy : 0.9727 speed/epoch_sec : 0.0359 speed/samples_per_sec : 1115.6622 \n",
      "epoch: 30\n",
      "train: CE : 0.0550 accuracy : 0.9848 speed/epoch_sec : 0.5022 speed/samples_per_sec : 467.9483 \n",
      "test: CE : 0.0864 accuracy : 0.9728 speed/epoch_sec : 0.0429 speed/samples_per_sec : 931.9692 \n",
      "epoch: 31\n",
      "train: CE : 0.0530 accuracy : 0.9856 speed/epoch_sec : 0.5298 speed/samples_per_sec : 443.5726 \n",
      "test: CE : 0.0851 accuracy : 0.9720 speed/epoch_sec : 0.0398 speed/samples_per_sec : 1004.0646 \n",
      "epoch: 32\n",
      "train: CE : 0.0510 accuracy : 0.9863 speed/epoch_sec : 0.5354 speed/samples_per_sec : 438.9483 \n",
      "test: CE : 0.0853 accuracy : 0.9725 speed/epoch_sec : 0.0420 speed/samples_per_sec : 952.1257 \n",
      "epoch: 33\n",
      "train: CE : 0.0496 accuracy : 0.9867 speed/epoch_sec : 0.5243 speed/samples_per_sec : 448.1980 \n",
      "test: CE : 0.0807 accuracy : 0.9738 speed/epoch_sec : 0.0386 speed/samples_per_sec : 1035.0300 \n",
      "epoch: 34\n",
      "train: CE : 0.0477 accuracy : 0.9870 speed/epoch_sec : 0.5385 speed/samples_per_sec : 436.4250 \n",
      "test: CE : 0.0882 accuracy : 0.9716 speed/epoch_sec : 0.0430 speed/samples_per_sec : 931.1002 \n",
      "epoch: 35\n",
      "train: CE : 0.0462 accuracy : 0.9877 speed/epoch_sec : 0.5607 speed/samples_per_sec : 419.1361 \n",
      "test: CE : 0.0847 accuracy : 0.9729 speed/epoch_sec : 0.0380 speed/samples_per_sec : 1052.1269 \n",
      "epoch: 36\n",
      "train: CE : 0.0449 accuracy : 0.9880 speed/epoch_sec : 0.5392 speed/samples_per_sec : 435.7925 \n",
      "test: CE : 0.0808 accuracy : 0.9734 speed/epoch_sec : 0.0449 speed/samples_per_sec : 891.7268 \n",
      "epoch: 37\n",
      "train: CE : 0.0435 accuracy : 0.9886 speed/epoch_sec : 0.5313 speed/samples_per_sec : 442.3041 \n",
      "test: CE : 0.0840 accuracy : 0.9728 speed/epoch_sec : 0.0429 speed/samples_per_sec : 931.6845 \n",
      "epoch: 38\n",
      "train: CE : 0.0420 accuracy : 0.9891 speed/epoch_sec : 0.5628 speed/samples_per_sec : 417.5847 \n",
      "test: CE : 0.0788 accuracy : 0.9750 speed/epoch_sec : 0.0468 speed/samples_per_sec : 854.7550 \n",
      "epoch: 39\n",
      "train: CE : 0.0408 accuracy : 0.9895 speed/epoch_sec : 0.5492 speed/samples_per_sec : 427.9181 \n",
      "test: CE : 0.0791 accuracy : 0.9747 speed/epoch_sec : 0.0448 speed/samples_per_sec : 892.7233 \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CE/eval</td><td>â–ˆâ–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>CE/train</td><td>â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>accuracy/eval</td><td>â–â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>accuracy/train</td><td>â–â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>speed/epoch_sec/eval</td><td>â–ƒâ–‚â–ˆâ–â–„â–‚â–„â–„â–ƒâ–…â–†â–…â–„â–†â–†â–…â–ƒâ–…â–„â–…â–ƒâ–…â–…â–…â–†â–…â–†â–„â–†â–„â–‡â–†â–†â–…â–‡â–…â–‡â–‡â–ˆâ–‡</td></tr><tr><td>speed/epoch_sec/train</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–…â–„â–„â–„â–…â–†â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–…â–†â–‡â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡</td></tr><tr><td>speed/samples_per_sec/eval</td><td>â–…â–†â–â–ˆâ–„â–†â–„â–„â–…â–ƒâ–‚â–ƒâ–„â–‚â–‚â–ƒâ–…â–ƒâ–„â–ƒâ–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–‚â–â–</td></tr><tr><td>speed/samples_per_sec/train</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–†â–‡â–…â–…â–…â–„â–„â–ƒâ–…â–…â–„â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–</td></tr><tr><td>sys/ram_gb</td><td>â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–„â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CE/eval</td><td>0.07913</td></tr><tr><td>CE/train</td><td>0.04081</td></tr><tr><td>accuracy/eval</td><td>0.9747</td></tr><tr><td>accuracy/train</td><td>0.98948</td></tr><tr><td>speed/epoch_sec/eval</td><td>0.04481</td></tr><tr><td>speed/epoch_sec/train</td><td>0.54917</td></tr><tr><td>speed/samples_per_sec/eval</td><td>892.72326</td></tr><tr><td>speed/samples_per_sec/train</td><td>427.91811</td></tr><tr><td>sys/ram_gb</td><td>2.08061</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mnist_test_1</strong> at: <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/99yotdm1' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/99yotdm1</a><br> View project at: <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250722_153949-99yotdm1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_nn(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
