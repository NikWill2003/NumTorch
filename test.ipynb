{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple, Self, Iterable\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6c182ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng()\n",
    "DTYPE = 'float64' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO \n",
    "- add in auto grad visulisation\n",
    "- add in a way to test the primative operations (make sure log, exp and rtruediv pass)\n",
    "- add in convolutions (possibly)\n",
    "- todo implement dataloaders \n",
    "- add logging, WandB and also terminal logging\n",
    "'''\n",
    "\n",
    "class Tensor():\n",
    "    def __init__(self, data, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data, dtype=DTYPE)\n",
    "        self.grad = np.zeros_like(data, dtype=DTYPE)\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int]:\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int: \n",
    "        return self.data.size\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = np.zeros_like(self.data, dtype=DTYPE)\n",
    "\n",
    "    def item(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "    def _unbroadcast(self, grad: np.ndarray) -> Self:\n",
    "        dims_to_remove = tuple(i for i in range(len(grad.shape) - len(self.shape))) \n",
    "        # remove prepended padding dimensions\n",
    "        grad = np.sum(grad, axis=dims_to_remove, keepdims=False) \n",
    "        dims_to_reduce = tuple(i for i, (d1,d2) in enumerate(zip(grad.shape, self.shape)) if d1!=d2)\n",
    "        # reduce broadcasted dimensions\n",
    "        return np.sum(grad, axis=dims_to_reduce, keepdims=True)\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self) -> None:\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            \n",
    "    def __add__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data + rhs.data, (self, rhs), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += self._unbroadcast(out.grad)\n",
    "            rhs.grad += rhs._unbroadcast(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self) -> Self:\n",
    "        out = Tensor(-self.data, (self,), 'neg')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, rhs) -> Self:\n",
    "        return self + (-rhs)\n",
    "\n",
    "    def __mul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data*rhs.data, (self, rhs), f'*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += self._unbroadcast(out.grad * rhs.data)\n",
    "            rhs.grad += rhs._unbroadcast(out.grad * self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, rhs) -> Self:\n",
    "        return self * (rhs**-1)\n",
    "      \n",
    "    def __pow__(self, rhs) -> Self: \n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        lhs_is_neg = self.data < 0\n",
    "        rhs_is_frac = ~np.isclose(rhs.data % 1, 0)\n",
    "        if np.any(lhs_is_neg & rhs_is_frac):\n",
    "            raise ValueError('cannot raise negative value to a decimal power')\n",
    "        \n",
    "        out = Tensor(self.data**rhs.data, (self, rhs), f'**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * ((rhs.data)*(self.data**(rhs.data-1)))\n",
    "            rhs.grad += out.grad * (self.data ** rhs.data) * np.log(rhs.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''data shape: (da, ..., d2, d1, n, k) rhs shape: (ob, ..., o2, o1, k, m)\n",
    "       inputs are broadcast so that they have the same shape by expanding along\n",
    "       dimensions if possible, out shape: (tc, ..., t2, t1, n, m), where ti = max(di, oi)\n",
    "       if di or oi does not exist it is treated as 1, and c = max d, a\n",
    "       if self is 1d shape is prepended with a 1, for rhs it would be appended'''\n",
    "    def __matmul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data @ rhs.data, (self, rhs), '@')\n",
    "\n",
    "        def _backward():\n",
    "            A, B, = self.data, rhs.data\n",
    "            g = out.grad\n",
    "            # broadcast 1d arrays to be 2d \n",
    "            A2 = A.reshape(1, -1) if len(A.shape) == 1 else A\n",
    "            B2 = B.reshape(-1, 1) if len(B.shape) == 1 else B\n",
    "            # extend g to have reduced dims\n",
    "            g = np.expand_dims(g, -1) if len(B.shape) == 1 else g\n",
    "            g = np.expand_dims(g, -2) if len(A.shape) == 1 else g\n",
    "            # transpose last 2 dimensions, as matmul treats tensors as batched matricies\n",
    "            self.grad += self._unbroadcast(g @ B2.swapaxes(-2, -1))\n",
    "            rhs.grad += rhs._unbroadcast(A2.swapaxes(-2, -1) @ g)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Self:\n",
    "        out = Tensor((self.data > 0) * self.data, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # need to check inp is non-negative\n",
    "    def log(self) -> Self:\n",
    "        if np.any(self.data < 0):\n",
    "            raise ValueError('cannot log negative values')\n",
    "        out = Tensor(np.log(self.data), (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 / self.data \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self) -> Self:\n",
    "        out = Tensor(np.exp(self.data), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += np.exp(self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None) -> Self:\n",
    "        out = Tensor(np.sum(self.data, axis=axis), (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "            self.grad += g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis=None) -> Self:\n",
    "        out = Tensor(np.mean(self.data, axis=axis), (self,), 'mean')\n",
    "\n",
    "        def _backward():\n",
    "            N =  self.size // out.size \n",
    "            g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "            self.grad += g / N\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rsub__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rmul__(self, lhs) -> Self:\n",
    "        return self * lhs\n",
    "    \n",
    "    def __rtruediv__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs / self\n",
    "    \n",
    "    def __rpow__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs ** self\n",
    "    \n",
    "    def __rmatmul__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs @ self\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape: tuple, bounds = (0,1)) -> Self:\n",
    "        lower, upper = bounds\n",
    "        data = RNG.random(shape, dtype=DTYPE)*(upper-lower) + lower\n",
    "        return cls(data)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'tensor shape: {self.shape}, op:{self._op}'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f977fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def kaiming(cls, fan_in, shape):\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        weights = RNG.standard_normal(shape, dtype=DTYPE)*std\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        return cls(np.zeros(shape, dtype=DTYPE))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'parameter shape: {self.shape}, size: {self.size}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9103a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    @property\n",
    "    def modules(self) -> list[Self]:\n",
    "        modules: list[Self] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, Module):\n",
    "                modules.append(value)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "\n",
    "            elif isinstance(value, Iterable) and not isinstance(value, (str, bytes)):\n",
    "                for v in value:\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "                    \n",
    "        return modules\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> list[Parameter]:\n",
    "        immediate_params = [attr for attr in self.__dict__.values() \n",
    "                                    if isinstance(attr, Parameter)]\n",
    "        modules_params = [param for module in self.modules \n",
    "                                    for param in module.params]\n",
    "        return immediate_params + modules_params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Affine(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.A = Parameter.kaiming(in_dim, (in_dim, out_dim))\n",
    "        self.b = Parameter.zeros((out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in), A : (in, out), B: out\n",
    "        return (x @ self.A) + self.b\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, x):\n",
    "        return x.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a67cdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxCrossEntropy():\n",
    "\n",
    "    def __call__(z: Tensor, y) -> Tensor:\n",
    "        '''logits z, shape (B, C), true lables y, shape (B, C)'''\n",
    "        loss = ((z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "        return loss\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params: list[Parameter], lr: float):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.data += -self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d3a259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimiser, loss, train_loader, test_loader, logger, wandb_run = None):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss = loss\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epoch = 1\n",
    "        self.logger = logger\n",
    "        self.wandb_run = wandb_run\n",
    "\n",
    "    def train_epoch():\n",
    "        pass\n",
    "\n",
    "    def validate():\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def log_metrics():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dd501775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (50, 100), size: 5000,\n",
       " parameter shape: (100,), size: 100,\n",
       " parameter shape: (100, 200), size: 20000,\n",
       " parameter shape: (200,), size: 200,\n",
       " parameter shape: (200, 10), size: 2000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "feedforward.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "529f4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' auto-grad testing suite\n",
    "    TODO:\n",
    "    - test all of the auto-grad primatives, \n",
    "    - test using central differences\n",
    "    - test by modifying each parameter individually i.e. only do scalar pertubations\n",
    "'''\n",
    "RNG = np.random.default_rng()\n",
    "if DTYPE=='float64':\n",
    "    EPS, ATOL, RTOL = 1e-6, 1e-5, 1e-3\n",
    "else:\n",
    "    EPS, ATOL, RTOL = 1e-4, 1e-4, 1e-2\n",
    "K = 20\n",
    "\n",
    "def compute_central_diff_error(test_fn, test_input, \n",
    "            other_inputs, eps, perturbed_idx, tols):\n",
    "    '''verify auto-grad of funciton f: R^n -> R'''\n",
    "    atol, rtol = tols\n",
    "\n",
    "    # rescale epsilon and convert to tensor\n",
    "    perturbed_val = test_input.data[perturbed_idx]\n",
    "    eps = eps * (1 + abs(perturbed_val))\n",
    "    pertubation_tensor = np.zeros_like(test_input.data, dtype=DTYPE)\n",
    "    pertubation_tensor[perturbed_idx] += eps \n",
    "    pertubation_tensor = Tensor(pertubation_tensor)\n",
    "\n",
    "    # Compute grad\n",
    "    for tensor in [test_input, *other_inputs]:\n",
    "        tensor.zero_grad()\n",
    "    clean_out = test_fn(test_input, other_inputs)\n",
    "    clean_out.backward()\n",
    "    auto_grad = test_input.grad[perturbed_idx]\n",
    "\n",
    "    # Compute central diff Grad approximaiton\n",
    "    test_forward = test_input + pertubation_tensor\n",
    "    forward_out = test_fn(test_forward, other_inputs).item()\n",
    "    test_back = test_input - pertubation_tensor\n",
    "    back_out = test_fn(test_back, other_inputs).item()\n",
    "    approx_grad = (forward_out - back_out) / (2*eps)\n",
    "\n",
    "\n",
    "    abs_err = abs(approx_grad - auto_grad)\n",
    "    rel_err = abs_err / (abs(auto_grad) + atol)\n",
    "    is_close = abs_err <= atol + rtol*abs(auto_grad)\n",
    "\n",
    "    return is_close, abs_err, rel_err, clean_out.item(), forward_out, back_out\n",
    "\n",
    "# need to generate inputs, compute cd err and output/format test result, to log file maybe?\n",
    "def test_fn_random_inputs(test_fn, test_shape, other_shapes=[], input_bounds=(-10, 10),\n",
    "                          num_samples=K, eps=EPS, tols=(ATOL, RTOL)):\n",
    "    \n",
    "    test_input = Tensor.random(test_shape, input_bounds)\n",
    "    other_inputs = [Tensor.random(shape, input_bounds) for shape in other_shapes]\n",
    "\n",
    "    num_samples = min(test_input.size, num_samples)\n",
    "    pertubation_nums = RNG.choice(test_input.size, size=num_samples, replace=False)\n",
    "    pretubation_idxs = np.unravel_index(pertubation_nums, test_shape)\n",
    "\n",
    "    all_close = True\n",
    "    failed = 0\n",
    "    # log = inspect.getsource(test_fn) + '\\n' \n",
    "    log = ''\n",
    "    log += f'test input \\n {test_input.data} \\nother inputs \\n'\n",
    "    for other_input in other_inputs:\n",
    "        log += f' {other_input.data} \\n'\n",
    "    for sample_i in range(num_samples):\n",
    "        perturbed_idx = tuple(int(pert_dim[sample_i]) for pert_dim in pretubation_idxs)\n",
    "        is_close, abs_err, rel_err, clean_out, forward_out, back_out = compute_central_diff_error(\n",
    "                                        test_fn, test_input, other_inputs, eps, perturbed_idx, tols)\n",
    "        log += f'test {'passed' if is_close else 'failed'}: abs err = {abs_err:.4f}, rel err = {rel_err:.4f}, perturbed idx = {perturbed_idx} \\n'\n",
    "        log += f'clean_out: {clean_out} forward_out: {forward_out} back_out: {back_out} \\n'\n",
    "        if not is_close:\n",
    "            all_close = False\n",
    "            failed += 1\n",
    "            # some logic for logging the failed case\n",
    "\n",
    "    return all_close, log\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fdca5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ufuncs = {'add' : lambda test_inp, other_inps: (test_inp+other_inps[0]).sum(),\n",
    "            'radd': lambda test_inp, other_inps: (other_inps[0]+test_inp).sum(),\n",
    "            'sub' : lambda test_inp, other_inps: (test_inp-other_inps[0]).sum(),\n",
    "            'rsub': lambda test_inp, other_inps: (other_inps[0]-test_inp).sum(),\n",
    "            'mul' : lambda test_inp, other_inps: (test_inp*other_inps[0]).sum(),\n",
    "            'rmul': lambda test_inp, other_inps: (other_inps[0]*test_inp).sum(),\n",
    "            # 'pow' : lambda test_inp, other_inps: (test_inp**other_inps[0]).sum(),\n",
    "            # 'rpow': lambda test_inp, other_inps: (other_inps[0]**test_inp).sum(),\n",
    "            'truediv' : lambda test_inp, other_inps: (test_inp/other_inps[0]).sum(),\n",
    "            'rtruediv': lambda test_inp, other_inps: (other_inps[0]/test_inp).sum(),\n",
    "            }\n",
    "\n",
    "matmul_fns = {'matmul': lambda test_inp, other_inps: (test_inp@other_inps[0]).sum(),\n",
    "              'rmatmul': lambda test_inp, other_inps: (other_inps[0]@test_inp).sum(),}\n",
    "\n",
    "unary_ufunc = {'relu': lambda test_inp, other_inps: (test_inp.relu()).sum(),\n",
    "            'log': lambda test_inp, other_inps: (test_inp.log()).sum(),\n",
    "            'exp': lambda test_inp, other_inps: (test_inp.exp()).sum(),\n",
    "            'sum': lambda test_inp, other_inps: test_inp.sum(),\n",
    "            'mean': lambda test_inp, other_inps: test_inp.mean(),}\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ce2720b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: relu passed\n",
      "function: log passed\n",
      "function: exp passed\n",
      "function: sum passed\n",
      "function: mean passed\n",
      "function: matmul passed\n",
      "function: rmatmul passed\n",
      "function: add passed\n",
      "function: radd passed\n",
      "function: sub passed\n",
      "function: rsub passed\n",
      "function: mul passed\n",
      "function: rmul passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikiw\\AppData\\Local\\Temp\\ipykernel_40488\\3619599098.py:102: RuntimeWarning: invalid value encountered in log\n",
      "  rhs.grad += out.grad * (self.data ** rhs.data) * np.log(rhs.data)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape () doesn't match the broadcast shape (2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func_name, test_fn \u001b[38;5;129;01min\u001b[39;00m bin_ufuncs.items():\n\u001b[32m     18\u001b[39m     test_shape, other_shapes = (\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m), [(\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfunction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mall_close\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_close:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtest_fn_random_inputs\u001b[39m\u001b[34m(test_fn, test_shape, other_shapes, input_bounds, num_samples, eps, tols)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m     66\u001b[39m     perturbed_idx = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(pert_dim[sample_i]) \u001b[38;5;28;01mfor\u001b[39;00m pert_dim \u001b[38;5;129;01min\u001b[39;00m pretubation_idxs)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     is_close, abs_err, rel_err, clean_out, forward_out, back_out = compute_central_diff_error(\n\u001b[32m     68\u001b[39m                                     test_fn, test_input, other_inputs, eps, perturbed_idx, tols)\n\u001b[32m     69\u001b[39m     log += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mis_close\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: abs err = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_err\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, rel err = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel_err\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, perturbed idx = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperturbed_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     70\u001b[39m     log += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mclean_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m forward_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforward_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m back_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mback_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mcompute_central_diff_error\u001b[39m\u001b[34m(test_fn, test_input, other_inputs, eps, perturbed_idx, tols)\u001b[39m\n\u001b[32m     28\u001b[39m     tensor.zero_grad()\n\u001b[32m     29\u001b[39m clean_out = test_fn(test_input, other_inputs)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m clean_out.backward()\n\u001b[32m     31\u001b[39m auto_grad = test_input.grad[perturbed_idx]\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Compute central diff Grad approximaiton\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m build_topo(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     node._backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mTensor.__pow__.<locals>._backward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mself\u001b[39m.grad += out.grad * ((rhs.data)*(\u001b[38;5;28mself\u001b[39m.data**(rhs.data-\u001b[32m1\u001b[39m)))\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     rhs.grad += out.grad * (\u001b[38;5;28mself\u001b[39m.data ** rhs.data) * np.log(rhs.data)\n",
      "\u001b[31mValueError\u001b[39m: non-broadcastable output operand with shape () doesn't match the broadcast shape (2,3)"
     ]
    }
   ],
   "source": [
    "for func_name, test_fn in unary_ufunc.items():\n",
    "    test_shape, other_shapes = (2, 3), [(3,2)]\n",
    "    input_bounds = (1, 100) if func_name == 'log' else (-100, 100)\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)\n",
    "\n",
    "for func_name, test_fn in matmul_fns.items():\n",
    "    test_shape = (2, 3) if func_name == 'matmul' else (3, 2)\n",
    "    other_shapes = [(3, 2)] if func_name == 'matmul' else [(2, 3)]\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)\n",
    "\n",
    "for func_name, test_fn in bin_ufuncs.items():\n",
    "    test_shape, other_shapes = (2, 3), [(2,3)]\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "test input \n",
      " [[-9.71146134 -8.89748529 -0.72254851 ...  6.45520874  8.76632988\n",
      "   0.14466156]\n",
      " [-5.72667199 -5.32969713 -0.88090023 ... -2.80718778  2.35732905\n",
      "  -9.82730865]\n",
      " [ 0.03189104  3.2142204  -6.39239274 ...  3.12862717 -1.14919706\n",
      "  -7.33857508]\n",
      " ...\n",
      " [ 4.49626838 -3.78306744  4.85917813 ...  3.27465866  7.58974102\n",
      "  -4.54765277]\n",
      " [ 4.63381445  5.7512736  -8.47256845 ... -1.67872051 -6.61579239\n",
      "  -3.042671  ]\n",
      " [ 6.90589474 -7.29616348  8.74373567 ...  0.9312988  -1.52038462\n",
      "  -1.96318709]] \n",
      "other inputs \n",
      " [-9.30978657] \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (23, 35) \n",
      "clean_out: -251.5008700293402 forward_out: -251.5008700339569 back_out: -251.50087002472355 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (7, 40) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086482080687 back_out: -251.50087523787354 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (47, 37) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086905136615 back_out: -251.5008710073143 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (46, 33) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086883951684 back_out: -251.50087121916357 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (16, 30) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087172363902 back_out: -251.5008683350414 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (22, 38) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087955561912 back_out: -251.50086050306135 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (24, 35) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087035564937 back_out: -251.50086970303107 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (28, 48) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086870107629 back_out: -251.50087135760413 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (1, 9) \n",
      "clean_out: -251.5008700293402 forward_out: -251.5008698566387 back_out: -251.5008702020417 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (7, 49) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087047210317 back_out: -251.50086958657727 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (11, 45) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086447996856 back_out: -251.50087557871183 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (45, 7) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087347319624 back_out: -251.5008665854843 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (32, 40) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086867180022 back_out: -251.5008713868802 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (14, 3) \n",
      "clean_out: -251.5008700293402 forward_out: -251.5008682967619 back_out: -251.5008717619185 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (30, 6) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086691255166 back_out: -251.50087314612875 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (16, 19) \n",
      "clean_out: -251.5008700293402 forward_out: -251.5008715318018 back_out: -251.5008685268786 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (10, 2) \n",
      "clean_out: -251.5008700293402 forward_out: -251.5008656936052 back_out: -251.50087436507516 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (5, 43) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50086965961106 back_out: -251.50087039906936 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (33, 28) \n",
      "clean_out: -251.5008700293402 forward_out: -251.50087125082985 back_out: -251.50086880785057 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (32, 0) \n",
      "clean_out: -251.5008700293402 forward_out: -251.500864567505 back_out: -251.5008754911754 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "test_shape, other_shapes = (50, 50), (1,)\n",
    "test_fn = lambda test, other: feedforward(test).sum()\n",
    "all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes)\n",
    "\n",
    "print(all_close)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2dc5cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.144422779277987\n",
      "[[0.65209104 1.09494424 0.08690697]\n",
      " [1.33157557 0.65336944 0.44156278]]\n",
      "[[-1.26670726 -0.04725824 -3.08710802]\n",
      " [-0.46560782 -0.68132569 -0.8277387 ]]\n"
     ]
    }
   ],
   "source": [
    "A = Tensor.random((2,3))\n",
    "C = Tensor.random((2,3))\n",
    "B = (A ** C).sum()\n",
    "B.backward()\n",
    "print(B.data)\n",
    "print(A.grad)\n",
    "print(C.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc265f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
