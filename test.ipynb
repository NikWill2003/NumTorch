{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple, Self, Iterable\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c182ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng()\n",
    "DTYPE = 'float64' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO \n",
    "- input and gradient sanitisation (ensure both are defined)\n",
    "- add in auto grad visulisation\n",
    "- add in a way to test the primative operations\n",
    "- add in convolutions\n",
    "- todo implement dataloaders \n",
    "- add logging, WandB and also terminal logging\n",
    "'''\n",
    "\n",
    "class Tensor():\n",
    "    def __init__(self, data, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data, dtype='float64')\n",
    "        self.grad = np.zeros_like(data, dtype='float64')\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int]:\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int: \n",
    "        return self.data.size\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = np.zeros_like(self.data, dtype='float64')\n",
    "\n",
    "    def item(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "    def _unbroadcast(self, grad: np.ndarray) -> Self:\n",
    "        dims_to_remove = tuple(i for i in range(len(grad.shape) - len(self.shape))) \n",
    "        # remove prepended padding dimensions\n",
    "        grad = np.sum(grad, axis=dims_to_remove, keepdims=False) \n",
    "        dims_to_reduce = tuple(i for i, (d1,d2) in enumerate(zip(grad.shape, self.shape)) if d1!=d2)\n",
    "        # reduce broadcasted dimensions\n",
    "        return np.sum(grad, axis=dims_to_reduce, keepdims=True)\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self) -> None:\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            \n",
    "    def __add__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data + rhs.data, (self, rhs), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += self._unbroadcast(out.grad)\n",
    "            rhs.grad += rhs._unbroadcast(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self) -> Self:\n",
    "        out = Tensor(-self.data, (self,), 'neg')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, rhs) -> Self:\n",
    "        return self + (-rhs)\n",
    "\n",
    "    def __mul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data*rhs.data, (self,), f'*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += self._unbroadcast(out.grad * rhs.data)\n",
    "            rhs.grad += rhs._unbroadcast(out.grad * self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, rhs) -> Self:\n",
    "        return self * (rhs**-1)\n",
    "      \n",
    "    # TODO need to restrict the rhs input when lhs contains negative values and check grad is defined\n",
    "    def __pow__(self, rhs) -> Self: \n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        # lhs_is_neg = np.all(self.data < 0)\n",
    "        # rhs_is_frac = \n",
    "        out = Tensor(self.data**rhs.data, (self,), f'**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * ((rhs.data)*(self.data**(rhs.data-1)))\n",
    "            rhs.grad += out.grad * (self.data ** rhs.data) * np.log(rhs.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''data shape: (da, ..., d2, d1, n, k) rhs shape: (ob, ..., o2, o1, k, m)\n",
    "       inputs are broadcast so that they have the same shape by expanding along\n",
    "       dimensions if possible, out shape: (tc, ..., t2, t1, n, m), where ti = max(di, oi)\n",
    "       if di or oi does not exist it is treated as 1, and c = max d, a\n",
    "       if self is 1d shape is prepended with a 1, for rhs it would be appended'''\n",
    "    def __matmul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data @ rhs.data, (self, rhs), '@')\n",
    "\n",
    "        def _backward():\n",
    "            A, B, = self.data, rhs.data\n",
    "            g = out.grad\n",
    "            # broadcast 1d arrays to be 2d \n",
    "            A2 = A.reshape(1, -1) if len(A.shape) == 1 else A\n",
    "            B2 = B.reshape(-1, 1) if len(B.shape) == 1 else B\n",
    "            # extend g to have reduced dims\n",
    "            g = np.expand_dims(g, -1) if len(B.shape) == 1 else g\n",
    "            g = np.expand_dims(g, -2) if len(A.shape) == 1 else g\n",
    "            # transpose last 2 dimensions, as matmul treats tensors as batched matricies\n",
    "            self.grad += self._unbroadcast(g @ B2.swapaxes(-2, -1))\n",
    "            rhs.grad += rhs._unbroadcast(A2.swapaxes(-2, -1) @ g)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Self:\n",
    "        out = Tensor((self.data > 0) * self.data, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # need to check inp is non-negative\n",
    "    def log(self) -> Self:\n",
    "        out = Tensor(np.log(self.data), (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = self.data ** -1\n",
    "        out.backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self) -> Self:\n",
    "        out = Tensor(np.exp(self.data), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.exp(self.data)\n",
    "        out.backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None) -> Self:\n",
    "        out = Tensor(np.sum(self.data, axis=axis), (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "            self.grad += g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis=None) -> Self:\n",
    "        out = Tensor(np.mean(self.data, axis=axis), (self,), 'mean')\n",
    "\n",
    "        def _backward():\n",
    "            N = out.size // self.size \n",
    "            g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "            self.grad += g / N\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rsub__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rmul__(self, lhs) -> Self:\n",
    "        return self * lhs\n",
    "    \n",
    "    def __rtruediv__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs / self\n",
    "    \n",
    "    def __rpow__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs ** self\n",
    "    \n",
    "    def __rmatmul__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs @ self\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape: tuple, bounds = (0,1)) -> Self:\n",
    "        lower, upper = bounds\n",
    "        data = RNG.random(shape, dtype=DTYPE)*(upper-lower) + lower\n",
    "        return cls(data)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'tensor shape: {self.shape}, op:{self._op}'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f977fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    @classmethod\n",
    "    def kaiming(cls, fan_in, shape):\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        weights = RNG.normal(0, std, shape)\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        return cls(np.zeros(shape, dtype=DTYPE))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'parameter shape: {self.shape}, size: {self.size}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9103a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    @property\n",
    "    def modules(self) -> list[Self]:\n",
    "        modules: list[Self] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, Module):\n",
    "                modules.append(value)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "\n",
    "            elif isinstance(value, Iterable) and not isinstance(value, (str, bytes)):\n",
    "                for v in value:\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "                    \n",
    "        return modules\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> list[Parameter]:\n",
    "        immediate_params = [attr for attr in self.__dict__.values() \n",
    "                                    if isinstance(attr, Parameter)]\n",
    "        modules_params = [param for module in self.modules \n",
    "                                    for param in module.params]\n",
    "        return immediate_params + modules_params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Affine(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.A = Parameter.kaiming(in_dim, (in_dim, out_dim))\n",
    "        self.b = Parameter.zeros((out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in), A : (in, out), B: out\n",
    "        return (x @ self.A) + self.b\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, x):\n",
    "        return x.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67cdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxCrossEntropy():\n",
    "\n",
    "    def __call__(z: Tensor, y) -> Tensor:\n",
    "        '''logits z, shape (B, C), true lables y, shape (B, C)'''\n",
    "        loss = ((z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "        return loss\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params: list[Parameter], lr: float):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.data += -self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimiser, loss, train_loader, test_loader, logger, wandb_run = None):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss = loss\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epoch = 1\n",
    "        self.logger = logger\n",
    "        self.wandb_run = wandb_run\n",
    "\n",
    "    def train_epoch():\n",
    "        pass\n",
    "\n",
    "    def validate():\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def log_metrics():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd501775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (50, 100), size: 5000,\n",
       " parameter shape: (100,), size: 100,\n",
       " parameter shape: (100, 200), size: 20000,\n",
       " parameter shape: (200,), size: 200,\n",
       " parameter shape: (200, 10), size: 2000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "feedforward.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529f4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' auto-grad testing suite\n",
    "    TODO:\n",
    "    - test all of the auto-grad primatives, \n",
    "    - test using central differences\n",
    "    - test by modifying each parameter individually i.e. only do scalar pertubations\n",
    "    - for now \n",
    "'''\n",
    "RNG = np.random.default_rng()\n",
    "EPS, ATOL, RTOL = 5e-7, 1e-6, 1e-3\n",
    "K = 20\n",
    "\n",
    "def compute_central_diff_error(test_fn, test_input, \n",
    "            other_inputs, eps, perturbed_idx, tols):\n",
    "    '''verify auto-grad of funciton f: R^n -> R'''\n",
    "    atol, rtol = tols\n",
    "\n",
    "    # rescale epsilon and convert to tensor\n",
    "    perturbed_val = test_input.data[perturbed_idx]\n",
    "    eps = eps * (1 + abs(perturbed_val))\n",
    "    pertubation_tensor = np.zeros_like(test_input.data, dtype='float64')\n",
    "    pertubation_tensor[perturbed_idx] += eps \n",
    "    pertubation_tensor = Tensor(pertubation_tensor)\n",
    "\n",
    "    # Compute grad\n",
    "    for tensor in [test_input, *other_inputs]:\n",
    "        tensor.zero_grad()\n",
    "    clean_out = test_fn(test_input, other_inputs)\n",
    "    clean_out.backward()\n",
    "    auto_grad = test_input.grad[perturbed_idx]\n",
    "\n",
    "    # Compute central diff Grad approximaiton\n",
    "    test_forward = test_input + pertubation_tensor\n",
    "    forward_out = test_fn(test_forward, other_inputs).item()\n",
    "    test_back = test_input - pertubation_tensor\n",
    "    back_out = test_fn(test_back, other_inputs).item()\n",
    "    approx_grad = (forward_out - back_out)/ (2*eps)\n",
    "\n",
    "\n",
    "    abs_err = abs(approx_grad - auto_grad)\n",
    "    rel_err = abs_err / (abs(auto_grad) + atol)\n",
    "    is_close = abs_err <= atol + rtol*abs(auto_grad)\n",
    "\n",
    "    return (is_close, abs_err, rel_err)\n",
    "\n",
    "# need to generate inputs, compute cd err and output/format test result, to log file maybe?\n",
    "def test_fn_random_inputs(test_fn, test_shape, other_shapes=[], input_bounds=(-10, 10),\n",
    "                          num_samples=K, eps=EPS, tols=(ATOL, RTOL)):\n",
    "    \n",
    "    test_input = Tensor.random(test_shape, input_bounds)\n",
    "    other_inputs = [Tensor.random(shape, input_bounds) for shape in other_shapes]\n",
    "\n",
    "    num_samples = min(test_input.size, num_samples)\n",
    "    pertubation_nums = RNG.choice(test_input.size, size=num_samples, replace=False)\n",
    "    pretubation_idxs = np.unravel_index(pertubation_nums, test_shape)\n",
    "\n",
    "    all_close = True\n",
    "    failed = 0\n",
    "    log = inspect.getsource(test_fn) + '\\n' \n",
    "    log += f'test input \\n {test_input.data} \\nother inputs \\n'\n",
    "    for other_input in other_inputs:\n",
    "        log += f' {other_input.data} \\n'\n",
    "    for sample_i in range(num_samples):\n",
    "        perturbed_idx = tuple(pert_dim[sample_i] for pert_dim in pretubation_idxs)\n",
    "        is_close, abs_err, rel_err = compute_central_diff_error(test_fn, test_input, \n",
    "                                        other_inputs, eps, perturbed_idx, tols)\n",
    "        log += f'test {'passed' if is_close else 'failed'}: abs err = {abs_err}, rel err = {rel_err}, perturbed idx = {perturbed_idx} \\n'\n",
    "        if not is_close:\n",
    "            all_close = False\n",
    "            failed += 1\n",
    "            # some logic for logging the failed case\n",
    "\n",
    "    return (all_close, log)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c421b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "test_fn = lambda test, other: (test.relu()).sum()\n",
      "\n",
      "test input \n",
      " [[ 4.36584424 -6.89218445  0.72963676]\n",
      " [ 4.53202843 -7.43184834 -4.14504382]] \n",
      "other inputs \n",
      " [-9.61942403  7.20280344 -9.81055221] \n",
      " [-2.68631656  4.12879115] \n",
      "test passed: abs err = 0.0, rel err = 0.0, perturbed idx = (np.int64(1), np.int64(2)) \n",
      "test passed: abs err = 1.321722731262298e-10, rel err = 1.3217214095408887e-10, perturbed idx = (np.int64(0), np.int64(2)) \n",
      "test passed: abs err = 2.2529045295982542e-10, rel err = 2.2529022766959776e-10, perturbed idx = (np.int64(1), np.int64(0)) \n",
      "test passed: abs err = 3.765547873513242e-10, rel err = 3.765544107969134e-10, perturbed idx = (np.int64(0), np.int64(0)) \n",
      "test passed: abs err = 0.0, rel err = 0.0, perturbed idx = (np.int64(1), np.int64(1)) \n",
      "test passed: abs err = 0.0, rel err = 0.0, perturbed idx = (np.int64(0), np.int64(1)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_shape, other_shapes = (2, 3), (3,2)\n",
    "test_fn = lambda test, other: (test.relu()).sum()\n",
    "all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes)\n",
    "\n",
    "print(all_close)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cdcb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "test_fn = lambda test, other: feedforward(test).sum()\n",
      "\n",
      "test input \n",
      " [[ 5.24869302 -7.32949073 -4.59527146 ...  0.41753385  3.62603279\n",
      "   4.30435232]\n",
      " [ 6.13197392  0.2182355  -8.03118554 ...  3.53622372 -2.29636021\n",
      "   2.96845961]\n",
      " [ 5.90608006  1.59010563  4.56163719 ...  1.71477364  2.18141702\n",
      "   3.83171408]\n",
      " ...\n",
      " [ 1.68096585 -9.90775878 -3.22444042 ... -0.40164268 -3.23570823\n",
      "  -8.56105366]\n",
      " [-3.28470248  7.18695545 -9.25924831 ...  7.26671642 -2.33127542\n",
      "  -7.11449082]\n",
      " [-2.7405392  -6.70604766  1.11649571 ...  5.16444245  7.65561468\n",
      "  -8.81066537]] \n",
      "other inputs \n",
      " [1.57497144] \n",
      "test passed: abs err = 1.1500378649387244e-08, rel err = 2.763515178640418e-08, perturbed idx = (np.int64(27), np.int64(8)) \n",
      "test passed: abs err = 3.767702483337132e-08, rel err = 2.9018792550391e-08, perturbed idx = (np.int64(14), np.int64(33)) \n",
      "test passed: abs err = 2.928049241956998e-08, rel err = 1.2276544279545665e-07, perturbed idx = (np.int64(4), np.int64(19)) \n",
      "test passed: abs err = 4.281972353081809e-09, rel err = 1.44388963468133e-07, perturbed idx = (np.int64(8), np.int64(6)) \n",
      "test passed: abs err = 2.9683442842598495e-08, rel err = 1.248559691276487e-07, perturbed idx = (np.int64(16), np.int64(47)) \n",
      "test passed: abs err = 1.5297955680004804e-09, rel err = 3.501491432900009e-09, perturbed idx = (np.int64(39), np.int64(48)) \n",
      "test passed: abs err = 5.937868918781675e-09, rel err = 5.736170925093473e-08, perturbed idx = (np.int64(9), np.int64(35)) \n",
      "test passed: abs err = 1.4147078619686226e-08, rel err = 1.1696422484895394e-08, perturbed idx = (np.int64(11), np.int64(17)) \n",
      "test passed: abs err = 2.5550339621815965e-09, rel err = 1.8254326933081709e-09, perturbed idx = (np.int64(33), np.int64(11)) \n",
      "test passed: abs err = 1.3071077510293239e-08, rel err = 2.91500630607758e-06, perturbed idx = (np.int64(26), np.int64(24)) \n",
      "test passed: abs err = 7.695065695756398e-09, rel err = 2.0561293201779708e-08, perturbed idx = (np.int64(49), np.int64(32)) \n",
      "test passed: abs err = 2.4118177177623323e-09, rel err = 1.2131302545345128e-08, perturbed idx = (np.int64(25), np.int64(18)) \n",
      "test passed: abs err = 5.466336938653171e-09, rel err = 1.027856817029106e-08, perturbed idx = (np.int64(28), np.int64(3)) \n",
      "test passed: abs err = 2.2889706574602542e-08, rel err = 1.190898963902473e-07, perturbed idx = (np.int64(44), np.int64(28)) \n",
      "test passed: abs err = 2.049591971142739e-08, rel err = 2.522280238272397e-07, perturbed idx = (np.int64(29), np.int64(10)) \n",
      "test passed: abs err = 7.128115986354544e-09, rel err = 4.771755400859808e-08, perturbed idx = (np.int64(28), np.int64(29)) \n",
      "test passed: abs err = 1.6488792248248174e-08, rel err = 6.723928676005455e-08, perturbed idx = (np.int64(2), np.int64(42)) \n",
      "test passed: abs err = 3.460582925773892e-08, rel err = 1.0855442395578703e-07, perturbed idx = (np.int64(6), np.int64(40)) \n",
      "test passed: abs err = 3.0968067776804276e-08, rel err = 1.0704807071563571e-07, perturbed idx = (np.int64(16), np.int64(4)) \n",
      "test passed: abs err = 1.3728711145688521e-07, rel err = 8.041992301159648e-08, perturbed idx = (np.int64(1), np.int64(17)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "test_shape, other_shapes = (50, 50), (1,)\n",
    "test_fn = lambda test, other: feedforward(test).sum()\n",
    "all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes)\n",
    "\n",
    "print(all_close)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae2b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: (2, 100), op:Relu\n"
     ]
    }
   ],
   "source": [
    "A = Tensor.random((2, 50))\n",
    "feedforward = Sequential([Affine(50, 100), Relu()])\n",
    "B = feedforward(A)\n",
    "print(B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
