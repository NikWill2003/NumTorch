{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    def __init__(self, data, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data)\n",
    "        self.grad = np.zeros_like(data)\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self): \n",
    "        return self.data.size\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "            \n",
    "    def __add__(self, operand):\n",
    "        operand = operand if isinstance(operand, Tensor) else Tensor(operand)\n",
    "        out = Tensor(self.data + operand.data, (self, operand), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            operand.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        out = Tensor(-self.data, (self,), '-')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, operand):\n",
    "        return self + (-operand)\n",
    "\n",
    "    # only support tensor ** scalar for now, not supporting reversed\n",
    "    def __mul__(self, operand):\n",
    "        if not isinstance(operand, float):\n",
    "            raise NotImplementedError(f'* not implemented between tensor and {type(operand)}')\n",
    "        out = Tensor(self.data*operand, (self,), f'{operand}*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = out.grad * operand\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    # only support tensor ** scalar for now, not supporting reversed\n",
    "    def __truediv__(self, operand):\n",
    "        if not isinstance(operand, float):\n",
    "            raise NotImplementedError(f'/ not implemented between tensor and {type(operand)}')\n",
    "        return self * (operand**-1)\n",
    "    \n",
    "    # only support tensor ** scalar for now, not supporting reversed\n",
    "    def __pow__(self, operand):\n",
    "        if not isinstance(operand, float):\n",
    "            raise NotImplementedError(f'** not implemented between tensor and {type(operand)}')\n",
    "        out = Tensor(self.data*operand, (self,), f'**{operand}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = out.grad * ((operand)*(self.data**(operand-1)))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, operand):\n",
    "        operand = operand if isinstance(operand, Tensor) else Tensor(operand)\n",
    "        out = Tensor(self.data @ operand.data, (self, operand), '@')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad @ operand.data.T\n",
    "            operand.grad += self.data.T @ out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor((self.data > 0) * self.data, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self):\n",
    "        out = Tensor(sum(self.data), (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.full_like(self.data, out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        return (self.sum)/float(self.size)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'tensor: {self.data}, grad: {self.grad}, op:{self._op}'\n",
    "\n",
    "class Relu():\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, Tensor):\n",
    "            return input.relu()\n",
    "        else:\n",
    "            raise TypeError('Input type: {input.type}, not supported')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.arange(4).reshape((2,2)))\n",
    "b = Tensor(np.arange(4).reshape((2,2)))\n",
    "c = Tensor(np.arange(4).reshape((2,2)))\n",
    "\n",
    "d = a @ b\n",
    "e = d + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f30e69c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor: [[0 1]\n",
       " [2 3]], grad: [[ 91 455]\n",
       " [ 91 455]], op:"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.backward()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1affff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor(np.array([-1,2,3,4,-12]))\n",
    "a.data\n",
    "b = a.sum()\n",
    "b.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9348d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng()\n",
    "\n",
    "'''generate random dimenstions for input size,\n",
    "test given operator, generate random input, run \n",
    "forward through it and a slightly shitfed version,\n",
    "ensure that the backward pass gradient matches the \n",
    "finite differences gradient buy only a small amount.'''\n",
    "def test_tensor_op(op='+'):\n",
    "    a, b, c = RNG.integers(size=(3))\n",
    "    input1 = RNG.random(size=((a,b)))\n",
    "    input2 = RNG.random(size=(b,c))\n",
    "\n",
    "    perturbed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3a259ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_finalize__',\n",
       " '__array_function__',\n",
       " '__array_interface__',\n",
       " '__array_namespace__',\n",
       " '__array_priority__',\n",
       " '__array_struct__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__buffer__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imatmul__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__xor__',\n",
       " 'all',\n",
       " 'any',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argpartition',\n",
       " 'argsort',\n",
       " 'astype',\n",
       " 'base',\n",
       " 'byteswap',\n",
       " 'choose',\n",
       " 'clip',\n",
       " 'compress',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'copy',\n",
       " 'ctypes',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'data',\n",
       " 'device',\n",
       " 'diagonal',\n",
       " 'dot',\n",
       " 'dtype',\n",
       " 'dump',\n",
       " 'dumps',\n",
       " 'fill',\n",
       " 'flags',\n",
       " 'flat',\n",
       " 'flatten',\n",
       " 'getfield',\n",
       " 'imag',\n",
       " 'item',\n",
       " 'itemset',\n",
       " 'itemsize',\n",
       " 'mT',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'newbyteorder',\n",
       " 'nonzero',\n",
       " 'partition',\n",
       " 'prod',\n",
       " 'ptp',\n",
       " 'put',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'repeat',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'round',\n",
       " 'searchsorted',\n",
       " 'setfield',\n",
       " 'setflags',\n",
       " 'shape',\n",
       " 'size',\n",
       " 'sort',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'strides',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'take',\n",
       " 'to_device',\n",
       " 'tobytes',\n",
       " 'tofile',\n",
       " 'tolist',\n",
       " 'tostring',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'var',\n",
       " 'view']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(np.ndarray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
