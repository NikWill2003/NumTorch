{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple, Self, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import wandb\n",
    "import time\n",
    "import psutil\n",
    "from typing import Optional, Literal\n",
    "import functools, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c182ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular constants\n",
    "RNG = np.random.default_rng()\n",
    "DTYPE = 'float32' \n",
    "\n",
    "proc  = psutil.Process(os.getpid())\n",
    "\n",
    "# testing constants\n",
    "if DTYPE=='float64':\n",
    "    EPS, ATOL, RTOL = 1e-6, 1e-5, 1e-3\n",
    "else:\n",
    "    EPS, ATOL, RTOL = 1e-4, 1e-4, 1e-2\n",
    "K = 20\n",
    "\n",
    "dtype_eps = {'float16': 1e-4,\n",
    "             'float32': 1e-7,\n",
    "             'float64': 1e-15}[DTYPE]\n",
    "\n",
    "Mode = Literal['train', 'eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    def __init__(self, data, requires_grad=False, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data, dtype=DTYPE)\n",
    "        self.grad = np.zeros_like(data, dtype=DTYPE)\n",
    "        self.requires_grad = requires_grad\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int]:\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int: \n",
    "        return self.data.size\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = np.zeros_like(self.data, dtype=DTYPE)\n",
    "\n",
    "    def item(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "    def _unbroadcast(self, grad: np.ndarray) -> np.ndarray:\n",
    "        dims_to_remove = tuple(i for i in range(len(grad.shape) - len(self.shape))) \n",
    "        # remove prepended padding dimensions\n",
    "        grad = np.sum(grad, axis=dims_to_remove, keepdims=False) \n",
    "        dims_to_reduce = tuple(i for i, (d1,d2) in enumerate(zip(grad.shape, self.shape)) if d1!=d2)\n",
    "        # reduce broadcasted dimensions\n",
    "        return np.sum(grad, axis=dims_to_reduce, keepdims=True)\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self) -> None:\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            node._prev = set(())\n",
    "            node._backward = lambda : None\n",
    "\n",
    "    def __getitem__(self, indexes):\n",
    "        out = Tensor(self.data[indexes], self.requires_grad, (self), 'slice')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                pass\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "            \n",
    "    def __add__(self, rhs) -> \"Tensor\":\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data + rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '+')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        out = Tensor(-self.data, self.requires_grad, (self,), 'neg')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, rhs) -> \"Tensor\":\n",
    "        return self + (-rhs)\n",
    "\n",
    "    def __mul__(self, rhs) -> \"Tensor\":\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data*rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), f'*')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * rhs.data)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, rhs) -> \"Tensor\":\n",
    "        return self * (rhs**-1)\n",
    "    \n",
    "    # TODO add check for rhs, if epxponent if negative the gradient is undefined\n",
    "    def __pow__(self, rhs) -> \"Tensor\": \n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        lhs_is_neg = self.data < 0\n",
    "        rhs_is_frac = ~np.isclose(rhs.data % 1, 0)\n",
    "        if np.any(lhs_is_neg & rhs_is_frac):\n",
    "            raise ValueError('cannot raise negative value to a decimal power')\n",
    "        \n",
    "        out = Tensor(self.data**rhs.data, self.requires_grad or rhs.requires_grad, (self,), f'**')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * ((rhs.data)*(self.data**(rhs.data-1))))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * (self.data ** rhs.data) * np.log(self.data))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''data shape: (da, ..., d2, d1, n, k) rhs shape: (ob, ..., o2, o1, k, m)\n",
    "       inputs are broadcast so that they have the same shape by expanding along\n",
    "       dimensions if possible, out shape: (tc, ..., t2, t1, n, m), where ti = max(di, oi)\n",
    "       if di or oi does not exist it is treated as 1, and c = max d, a\n",
    "       if self is 1d shape is prepended with a 1, for rhs it would be appended'''\n",
    "    def __matmul__(self, rhs) -> \"Tensor\":\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data @ rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '@')\n",
    "\n",
    "        def _backward():\n",
    "            A, B, = self.data, rhs.data\n",
    "            g = out.grad\n",
    "            # broadcast 1d arrays to be 2d \n",
    "            A2 = A.reshape(1, -1) if len(A.shape) == 1 else A\n",
    "            B2 = B.reshape(-1, 1) if len(B.shape) == 1 else B\n",
    "            # extend g to have reduced dims\n",
    "            g = np.expand_dims(g, -1) if len(B.shape) == 1 else g\n",
    "            g = np.expand_dims(g, -2) if len(A.shape) == 1 else g\n",
    "            # transpose last 2 dimensions, as matmul treats tensors as batched matricies\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(g @ B2.swapaxes(-2, -1))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(A2.swapaxes(-2, -1) @ g)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> \"Tensor\":\n",
    "        out = Tensor((self.data > 0) * self.data, self.requires_grad, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self) -> \"Tensor\":\n",
    "        if np.any(self.data < 0):\n",
    "            raise ValueError('cannot log negative values')\n",
    "        out = Tensor(np.log(self.data), self.requires_grad, (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self) -> \"Tensor\":\n",
    "        out = Tensor(np.exp(self.data), self.requires_grad, (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=False) -> \"Tensor\":\n",
    "        out = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims), self.requires_grad, (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                g = np.expand_dims(out.grad, axis) if (axis is not None and not keepdims) else out.grad\n",
    "                self.grad += g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis=None) -> \"Tensor\":\n",
    "        out = Tensor(np.mean(self.data, axis=axis), self.requires_grad, (self,), 'mean')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                N =  self.size // out.size \n",
    "                g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "                self.grad += g / N\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def clamp(self, a_min=None, a_max=None):\n",
    "        out = Tensor(np.clip(self.data, a_min=a_min, a_max=a_max), self.requires_grad, (self,), 'clamp')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                mask = (self.data > a_min) if a_min is not None else np.ones_like(self.data)\n",
    "                mask = mask & (self.data < a_max) if a_max is not None else mask\n",
    "                self.grad += out.grad * mask\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, lhs) -> \"Tensor\":\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rsub__(self, lhs) -> \"Tensor\":\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rmul__(self, lhs) -> \"Tensor\":\n",
    "        return self * lhs\n",
    "    \n",
    "    def __rtruediv__(self, lhs) -> \"Tensor\":\n",
    "        return Tensor(lhs) / self\n",
    "    \n",
    "    def __rpow__(self, lhs) -> \"Tensor\":\n",
    "        return Tensor(lhs) ** self\n",
    "    \n",
    "    def __rmatmul__(self, lhs) -> \"Tensor\":\n",
    "        return Tensor(lhs) @ self\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape: tuple, bounds = (0,1), requires_grad=False) -> \"Tensor\":\n",
    "        lower, upper = bounds\n",
    "        data = RNG.random(shape, dtype=DTYPE)*(upper-lower) + lower\n",
    "        return cls(data, requires_grad=requires_grad)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'tensor shape: {self.shape}, op:{self._op}'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f977fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data, requires_grad=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def kaiming(cls, fan_in, shape):\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        weights = (RNG.standard_normal(shape, dtype='float64')*std).astype(dtype=DTYPE)\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        return cls(np.zeros(shape, dtype=DTYPE))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'parameter shape: {self.shape}, size: {self.size}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9103a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class Module():\n",
    "    \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    @property\n",
    "    def modules(self) -> list[\"Module\"]:\n",
    "        modules: list[Module] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, Module):\n",
    "                modules.append(value)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "\n",
    "            elif isinstance(value, Iterable) and not isinstance(value, (str, bytes)):\n",
    "                for v in value:\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "                    \n",
    "        return modules\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> list[Parameter]:\n",
    "        immediate_params = [attr for attr in self.__dict__.values() \n",
    "                                    if isinstance(attr, Parameter)]\n",
    "        modules_params = [param for module in self.modules \n",
    "                                    for param in module.params]\n",
    "        return immediate_params + modules_params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def train(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for module in self.modules:\n",
    "            if isinstance(module, DynamicModule):\n",
    "                module.mode = 'train'\n",
    "        \n",
    "    def eval(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for module in self.modules:\n",
    "            if isinstance(module, DynamicModule):\n",
    "                module.mode = 'eval'\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Affine(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.A = Parameter.kaiming(in_dim, (in_dim, out_dim))\n",
    "        self.b = Parameter.zeros((out_dim))\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        x = input\n",
    "        # x: (B, in), A : (in, out), B: out\n",
    "        return (x @ self.A) + self.b\n",
    "    \n",
    "class DynamicModule(Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.mode = 'train'\n",
    "\n",
    "class DropOut(DynamicModule):\n",
    "    def __init__(self, p):\n",
    "        self.mode = 'train'\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.mode == 'eval':\n",
    "            return x * (1-self.p) # have to rescale during inference\n",
    "        mask_idx_nums = RNG.choice(x.size, size=int(x.size*(1-self.p)), replace=False)\n",
    "        mask_idxs = np.unravel_index(mask_idx_nums, x.shape)\n",
    "        mask = np.zeros_like(x.data)\n",
    "        mask[mask_idxs] = 1\n",
    "        \n",
    "        return x * mask\n",
    "    \n",
    "    '''TODO: can implement in the furture to make it faster once __getitem__ is implemented'''\n",
    "    # def forward(self, x: Tensor):\n",
    "    #     if self.mode == 'eval':\n",
    "    #         return x * (1-self.p) # have to rescale during inference\n",
    "    #     mask_idx_nums = RNG.choice(x.size, size=int(x.size*self.p), replace=False)\n",
    "    #     mask_idxs = np.unravel_index(mask_idx_nums, x.shape)\n",
    "    #     x.data[mask_idxs] = 0\n",
    "        \n",
    "    #     return x\n",
    "\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.relu()\n",
    "    \n",
    "class SoftMax(Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        # temporary as max is not an implemented op\n",
    "        x = x - np.max(x.data, axis=-1, keepdims=True) # for numerical stability \n",
    "        x = x.exp()\n",
    "        norm_c = x.sum(axis=-1, keepdims=True)\n",
    "        return x / norm_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b070de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "do = DropOut(0.5)\n",
    "x = Tensor.random((2,3))\n",
    "# print(A, do(A))\n",
    "mask_idx_nums = RNG.choice(x.size, size=int(x.size*(1-0.5)), replace=False)\n",
    "mask_idxs = np.unravel_index(mask_idx_nums, x.shape)\n",
    "mask = np.zeros_like(x.data)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67cdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(array, num_c):\n",
    "    one_hot = np.zeros(shape=(array.size, num_c))\n",
    "    for idx, i in enumerate(array):\n",
    "        one_hot[idx, i] = 1\n",
    "    return one_hot\n",
    "\n",
    "class Loss_Fn():\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError(\"Loss function must implement __call__ method\")\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "class SoftMaxCrossEntropy(Loss_Fn):\n",
    "    def __call__(self, z: Tensor, y) -> Tensor:\n",
    "        '''logits z, shape (B, C), true integer lables y, shape (B)'''\n",
    "        # TODO change from manual one hot encoding when getitem is implemented in tensor\n",
    "        y = Tensor(one_hot_encode(y, z.shape[-1])) #shape (B, C)\n",
    "        z = z - np.max(z.data, axis=-1, keepdims=True) # for numerical stability \n",
    "        loss = (-(z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "        return loss\n",
    "\n",
    "class CrossEntropy(Loss_Fn):\n",
    "    def __call__(self, q: Tensor, y) -> Tensor:\n",
    "        '''pred q, shape (B, C), true integer lables y, shape (B)'''\n",
    "        # TODO change from manual one hot encoding when getitem is implemented in tensor\n",
    "        y = Tensor(one_hot_encode(y, q.shape[-1])) #shape (B, C)\n",
    "        loss = -(y * (q+dtype_eps).log()).sum(axis=-1).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class MeanSquaredError():\n",
    "    def __call__(self, q: Tensor, y) -> Tensor:\n",
    "        '''pred q, shape (B, C), true values y, shape (B, C)'''\n",
    "        loss = ((q - y) ** 2).sum(axis=-1).mean()\n",
    "        return loss\n",
    "\n",
    "class optimiser():\n",
    "    def __init__(self, params: list[Parameter], lr: float=0.005):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def train(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def eval(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "class SGD(optimiser):\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for param in self.params:\n",
    "            if not param.requires_grad:\n",
    "                continue \n",
    "            param.data += -self.lr * param.grad\n",
    "\n",
    "class Adam(optimiser):\n",
    "    def __init__(self, params: list[Parameter], lr: float=0.005, \n",
    "                 betas: Tuple[float, float]=(0.9, 0.999), eps: float=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.b1 , self.b2 = betas\n",
    "        self.eps = eps\n",
    "        self.time_step = 0\n",
    "        self.m = [np.zeros_like(param.data, dtype=DTYPE) for param in params]\n",
    "        self.v = [np.zeros_like(param.data, dtype=DTYPE) for param in params]\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        self.time_step += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            if not p.requires_grad:\n",
    "                continue \n",
    "\n",
    "            g = p.grad\n",
    "            self.m[i] = self.b1*self.m[i] + (1-self.b1)*g\n",
    "            self.v[i] = self.b2*self.v[i] + (1-self.b2)*(g**2)\n",
    "            m_hat = self.m[i]/(1-self.b1**self.time_step)\n",
    "            v_hat = self.v[i]/(1-self.b2**self.time_step)\n",
    "\n",
    "            p.data += -self.lr * m_hat / (v_hat ** 0.5 + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9929b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, input_data, true_data, batch_size, shuffle=False, rng: np.random.Generator=RNG):\n",
    "        assert input_data.shape[0] == true_data.shape[0], 'must have the same number of inputs and true outputs'\n",
    "        self.X = input_data\n",
    "        self.y = true_data\n",
    "        self.N = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = rng\n",
    "\n",
    "    def __iter__(self):\n",
    "        X, y = self.X, self.y\n",
    "        if self.shuffle:\n",
    "            permutation = self.rng.permutation(X.shape[0])\n",
    "            X = X[permutation]\n",
    "            y = y[permutation]\n",
    "        splits = np.arange(self.N, X.shape[0], self.N)\n",
    "        X = np.split(X, splits, axis=0)\n",
    "        X = [Tensor(x, requires_grad=False) for x in X]\n",
    "        y = np.split(y, splits, axis=0)\n",
    "        return zip(X, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # samples/batch size rounded up\n",
    "        return ceil(self.X.shape[0]/self.N)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6539cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request, numpy as np\n",
    "# import os\n",
    "\n",
    "# os.makedirs('datasets')\n",
    "\n",
    "# url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "local_path = r\"datasets/mnist.npz\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, local_path)   # â‡¦ makes a real file\n",
    "data = np.load(local_path)\n",
    "\n",
    "# im = X_train[0:3]\n",
    "# print(type(im))\n",
    "# plt.imshow(im, cmap='grey')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031abe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = data[\"x_train\"][0:12].reshape((-1,784)) / 255, data[\"y_train\"][0:12]\n",
    "# print(y_train.shape)\n",
    "# train_loader = DataLoader(X_train, y_train, 4, shuffle=True)\n",
    "# for X, y in train_loader:\n",
    "#     print(y)\n",
    "#     im = X[0].reshape((28,28))\n",
    "#     print(type(im))\n",
    "#     plt.imshow(im, cmap='grey')\n",
    "#     plt.show()\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "725b5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(array, num_c):\n",
    "    one_hot = np.zeros(shape=(array.size, num_c))\n",
    "    for idx, i in enumerate(array):\n",
    "        one_hot[idx, i] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fc738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (784, 100), size: 78400,\n",
       " parameter shape: (100,), size: 100,\n",
       " parameter shape: (100, 200), size: 20000,\n",
       " parameter shape: (200,), size: 200,\n",
       " parameter shape: (200, 10), size: 2000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Sequential([Affine(784, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10), SoftMax()])\n",
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAverageMeter():\n",
    "    def __init__(self):\n",
    "        self._means = {}\n",
    "        self._counts = {}\n",
    "\n",
    "    def update_one(self, metric: str, mean: float, n: int = 1):  \n",
    "        if metric in self._means.keys():\n",
    "            k = self._counts[metric]\n",
    "            self._means[metric] += n*(mean - self._means[metric])/(k+n)\n",
    "            self._counts[metric] += n\n",
    "        else:\n",
    "            self._means[metric] = mean\n",
    "            self._counts[metric] = n\n",
    "\n",
    "    def update_many(self, metric_dict: dict[str, float|list]):\n",
    "        for metric, val in metric_dict.items():\n",
    "            if isinstance(val, float):\n",
    "                mean = val\n",
    "                n = 1\n",
    "            elif isinstance(val, list):\n",
    "                mean = val[0]\n",
    "                n = val[1]\n",
    "            self.update_one(metric, mean, n)\n",
    "    \n",
    "    def get_metric(self, metric):\n",
    "        if metric in self._means:\n",
    "            return  self._means[metric]\n",
    "        raise KeyError(f'{metric} not found')\n",
    "    \n",
    "    def dump_metrics(self):\n",
    "        return self._means\n",
    "\n",
    "    def reset(self, metric=None):\n",
    "        if metric is None:\n",
    "            self._means = {}\n",
    "            self._counts = {}\n",
    "        else:\n",
    "            del self._means[metric]\n",
    "            del self._counts[metric]\n",
    "\n",
    "    def get_log_str(self, metrics=None):\n",
    "        log_str = ''\n",
    "        metrics = self._means.keys() if metrics is None else metrics\n",
    "        for metric in metrics:\n",
    "            if metric not in self._means:\n",
    "                continue\n",
    "            log_str += f'{metric} : {self._means[metric]:.4f} '\n",
    "        return log_str\n",
    "\n",
    "    def __getitem__(self, key):  \n",
    "        return self.get_metric(key)\n",
    "    \n",
    "    def __contains__(self, key): \n",
    "        return key in self._means\n",
    "    \n",
    "    def __iter__(self):          \n",
    "        return iter(self._means)\n",
    "    \n",
    "    def items(self):            \n",
    "        return self._means.items()\n",
    "    \n",
    "    def __len__(self):           \n",
    "        return len(self._means)\n",
    "    \n",
    "    def __repr__(self):          \n",
    "        return f\"MultiAverageMeter({self._means})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814187de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {'project': 'torch from scratch testing',\n",
    "                'name': 'dropout_tests',\n",
    "                'config': {'optimiser':'adam', 'lr':0.05},\n",
    "                'group': 'mnist tests',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO:\n",
    " - early stopping\n",
    " - overfit batch\n",
    " '''\n",
    "\n",
    "Mode = Literal['train', 'eval']\n",
    "\n",
    "class Callback:\n",
    "    \n",
    "    def on_train_start(self, trainer): pass\n",
    "    def on_epoch_start(self, trainer): pass\n",
    "    def on_batch_start(self, trainer): pass\n",
    "    def on_batch_end(self, trainer): pass\n",
    "    def on_epoch_end(self, trainer): pass\n",
    "    def on_train_end(self, trainer): pass\n",
    "\n",
    "class CallbackList:\n",
    "    def __init__(self, callbacks):\n",
    "        self._callbacks = callbacks\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        def callbacks(*args, **kwargs):\n",
    "            for cb in self._callbacks:\n",
    "                method = getattr(cb, name, None)\n",
    "                if callable(method):\n",
    "                    method(*args, **kwargs)\n",
    "        return callbacks\n",
    "    \n",
    "def track_runtime(name=None):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(self: Trainer, *args, **kwargs):\n",
    "            start = time.perf_counter()\n",
    "            result = func(self, *args, **kwargs)\n",
    "            end = time.perf_counter()\n",
    "            metric_name = name or f'time/{func.__name__}'\n",
    "            self._meter.update_one(metric_name, end - start)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(\n",
    "            self, \n",
    "            model: Module, \n",
    "            optimiser: optimiser, \n",
    "            loss_fn: Loss_Fn, \n",
    "            train_loader: DataLoader, \n",
    "            validation_loader: DataLoader, \n",
    "            test_loader: DataLoader, \n",
    "            logger, \n",
    "            callbacks: list[Callback]):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.test_loader = test_loader\n",
    "        self._epoch = 1\n",
    "        self.logger = logger\n",
    "        self._meter = MultiAverageMeter()\n",
    "        self._mode: Mode = 'train'\n",
    "        self._callbacks = CallbackList(callbacks) \n",
    "\n",
    "    @track_runtime()\n",
    "    def train_batch(self, X, y):\n",
    "        self.optimiser.zero_grad()\n",
    "        output = self.model(X)\n",
    "        loss = self.loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "\n",
    "        return (loss, output)\n",
    "\n",
    "    @track_runtime()\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        self._callbacks.on_epoch_start(self)\n",
    "\n",
    "        for X, y in self.train_loader:\n",
    "\n",
    "            self._callbacks.on_batch_start(self)\n",
    "\n",
    "            out = self.evaluate_batch(X, y)\n",
    "            metrics_dict = {'loss': (out[0].item(), X.shape[0])} \n",
    "            self._meter.update_many(metrics_dict)\n",
    "\n",
    "            self._callbacks.on_batch_end()\n",
    "\n",
    "        self._callbacks.on_epoch_end(self)\n",
    "        return self._meter.get_log_str()\n",
    "\n",
    "    @track_runtime()\n",
    "    def evaluate_batch(self, X, y):\n",
    "        self.optimiser.zero_grad()\n",
    "        output = self.model(X)\n",
    "        loss = self.loss_fn(output, y)\n",
    "\n",
    "        return (loss, output)\n",
    "\n",
    "    @track_runtime\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        self._callbacks.on_epoch_start(self)\n",
    "\n",
    "        for X, y in self.validation_loader:\n",
    "\n",
    "            self._callbacks.on_batch_start(self)\n",
    "\n",
    "            out = self.evaluate_batch(X, y)\n",
    "            metrics_dict = {'loss': (out[0].item(), X.shape[0])} \n",
    "            self._meter.update_many(metrics_dict)\n",
    "\n",
    "            self._callbacks.on_batch_end(self)\n",
    "        \n",
    "        self._callbacks.on_epoch_end(self)\n",
    "        return self._meter.get_log_str()\n",
    "    \n",
    "    @track_runtime()\n",
    "    def test_batch(self, X:Tensor, y):\n",
    "        self.optimiser.zero_grad()\n",
    "        output = self.model(X)\n",
    "        loss = self.loss_fn(output, y)\n",
    "\n",
    "        return (loss, output)   \n",
    "\n",
    "    @track_runtime()\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        self._callbacks.on_epoch_start(self)\n",
    "\n",
    "        for X, y in self.test_loader:\n",
    "\n",
    "            self._callbacks.on_batch_start(self)\n",
    "\n",
    "            out = self.evaluate_batch(X, y)\n",
    "            metrics_dict = {'loss': (out[0].item(), X.shape[0])} \n",
    "            self._meter.update_many(metrics_dict)\n",
    "\n",
    "            self._callbacks.on_batch_end(self)\n",
    "        \n",
    "        self._callbacks.on_epoch_end(self)\n",
    "        return self._meter.get_log_str()\n",
    "\n",
    "    @track_runtime()\n",
    "    def train(self, epochs: int):\n",
    "\n",
    "        self._callbacks.on_train_start(self)\n",
    "\n",
    "        for t in range(epochs):\n",
    "            print(f'epoch: {t}')\n",
    "            self._meter.reset()\n",
    "            log = self.train_epoch()\n",
    "            print('train: ' + log)\n",
    "\n",
    "\n",
    "            self._meter.reset()\n",
    "            log = self.evaluate()\n",
    "            print('eval: ' + log)\n",
    "\n",
    "        if self.test_loader is not None:\n",
    "            self._meter.reset()\n",
    "            log = self.test()\n",
    "            print('eval: ' + log)\n",
    "\n",
    "        self._callbacks.on_train_end(self)\n",
    "\n",
    "\n",
    "class WandBCallback(Callback):\n",
    "    def __init__(self, api_key:Optional[str]=None):\n",
    "\n",
    "        self._api_key = api_key if api_key is not None else self.get_api_key()\n",
    "        assert api_key is not None, 'api key required'\n",
    "\n",
    "        self._mode: Mode = 'train'\n",
    "\n",
    "    def get_api_key(self):\n",
    "        load_dotenv()\n",
    "        return os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "    def on_train_start(self, trainer: Trainer):\n",
    "        self._meter = trainer._meter\n",
    "        wandb.login(key=self._api_key)\n",
    "        self._wandb_run = wandb.init(**wandb_config)\n",
    "\n",
    "    def on_epoch_start(self, trainer: Trainer): \n",
    "        self._mode = trainer._mode\n",
    "\n",
    "    def on_epoch_end(self, trainer): \n",
    "        metrics = {}\n",
    "        for name, val in self._meter.items():\n",
    "            metrics[name + f'/{self._mode}'] = val\n",
    "        metrics = {\n",
    "            'sys/ram_gb' : proc.memory_info().rss / 1_073_741_824\n",
    "        }\n",
    "        self._wandb_run.log(metrics, step=trainer._epoch)\n",
    "\n",
    "    def on_train_end(self, trainer): \n",
    "        self._wandb_run.finish()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff5d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_step(meter:MultiAverageMeter, train, nn, loader, loss_fn, optimiser):\n",
    "    start = time.time()\n",
    "    if train:\n",
    "        nn.train()\n",
    "    else:\n",
    "        nn.eval()\n",
    "\n",
    "    for X, y in loader:\n",
    "\n",
    "        nn.zero_grad()\n",
    "        out = nn(X)\n",
    "        loss = loss_fn(out, y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        preds = np.argmax(out.item(), axis=-1)\n",
    "        acc = np.sum(preds == y) / preds.size \n",
    "        \n",
    "        meter.update('CE', loss.item(), y.shape[0])\n",
    "        meter.update('accuracy', acc, y.shape[0])\n",
    "    end = time.time()\n",
    "    meter.update('speed/epoch_sec', end - start)\n",
    "    meter.update('speed/samples_per_sec', len(loader) / (end - start))\n",
    "    return meter.get_log_str()\n",
    "\n",
    "def train_nn(epochs, p=0.0):\n",
    "\n",
    "    meter = MultiAverageMeter()\n",
    "    wandb_logger = WandBLogger(meter, wandb_config)\n",
    "\n",
    "    X_train, y_train = data[\"x_train\"].reshape((-1,784)) / 255, data[\"y_train\"]\n",
    "    X_test, y_test = data[\"x_test\"].reshape((-1,784)) / 255, data[\"y_test\"]\n",
    "\n",
    "    train_loader = DataLoader(X_train, y_train, 256, shuffle=True)\n",
    "    test_loader = DataLoader(X_test, y_test, 256, shuffle=False)\n",
    "\n",
    "    nn = Sequential([DropOut(p), Affine(784, 200), DropOut(p), Relu(), Affine(200, 100), Relu(), Affine(100, 50), Relu(), Affine(50, 10), SoftMax()])\n",
    "    # nn = Sequential([Affine(784, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10), SoftMax()])\n",
    "    loss_fn = CrossEntropy()\n",
    "    optimiser = Adam(nn.params)\n",
    "    # optimiser = SGD(nn.params, lr=0.05)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f'epoch: {t}')\n",
    "        wandb_logger.train()\n",
    "        meter.reset()\n",
    "        log = train_test_step(meter, True, nn, train_loader, loss_fn, optimiser)\n",
    "        print('train: ' + log)\n",
    "        wandb_logger.log_epoch(t)\n",
    "\n",
    "        wandb_logger.eval()\n",
    "        meter.reset()\n",
    "        log = train_test_step(meter, False, nn, test_loader, loss_fn, optimiser)\n",
    "        print('test: ' + log)\n",
    "        wandb_logger.log_epoch(t)\n",
    "\n",
    "    wandb_logger.finish()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2335e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nik/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikiwillems9\u001b[0m (\u001b[33mnikiwillems9-university-of-bristol\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nik/personal_projects/dqn_from_scratch/wandb/run-20250801_224046-2wg366h6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/2wg366h6' target=\"_blank\">dropout_tests</a></strong> to <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/2wg366h6' target=\"_blank\">https://wandb.ai/nikiwillems9-university-of-bristol/torch%20from%20scratch%20testing/runs/2wg366h6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train: CE : 0.2510 accuracy : 0.9238 speed/epoch_sec : 4.7142 speed/samples_per_sec : 49.8497 \n",
      "test: CE : 0.1269 accuracy : 0.9603 speed/epoch_sec : 0.1202 speed/samples_per_sec : 332.7743 \n",
      "epoch: 1\n",
      "train: CE : 0.1025 accuracy : 0.9684 speed/epoch_sec : 5.8914 speed/samples_per_sec : 39.8886 \n",
      "test: CE : 0.0928 accuracy : 0.9706 speed/epoch_sec : 0.1649 speed/samples_per_sec : 242.5838 \n",
      "epoch: 2\n",
      "train: CE : 0.0685 accuracy : 0.9787 speed/epoch_sec : 5.7389 speed/samples_per_sec : 40.9489 \n",
      "test: CE : 0.1112 accuracy : 0.9683 speed/epoch_sec : 0.1859 speed/samples_per_sec : 215.1455 \n",
      "epoch: 3\n",
      "train: CE : 0.0537 accuracy : 0.9836 speed/epoch_sec : 5.8416 speed/samples_per_sec : 40.2288 \n",
      "test: CE : 0.0917 accuracy : 0.9736 speed/epoch_sec : 0.1795 speed/samples_per_sec : 222.8867 \n",
      "epoch: 4\n",
      "train: CE : 0.0514 accuracy : 0.9838 speed/epoch_sec : 22.2729 speed/samples_per_sec : 10.5509 \n",
      "test: CE : 0.0944 accuracy : 0.9734 speed/epoch_sec : 0.6020 speed/samples_per_sec : 66.4441 \n",
      "epoch: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m train_nn(\u001b[32m40\u001b[39m, p=\u001b[32m0.05\u001b[39m)\n\u001b[32m      3\u001b[39m train_nn(\u001b[32m40\u001b[39m, p=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_nn\u001b[39m\u001b[34m(epochs, p)\u001b[39m\n\u001b[32m     46\u001b[39m wandb_logger.train()\n\u001b[32m     47\u001b[39m meter.reset()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m log = \u001b[43mtrain_test_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtrain: \u001b[39m\u001b[33m'\u001b[39m + log)\n\u001b[32m     50\u001b[39m wandb_logger.log_epoch(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_test_step\u001b[39m\u001b[34m(meter, train, nn, loader, loss_fn, optimiser)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     10\u001b[39m     nn.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     out = \u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     loss = loss_fn(out, y)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     64\u001b[39m x = \u001b[38;5;28minput\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mDropOut.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     93\u001b[39m mask = np.zeros_like(x.data)\n\u001b[32m     94\u001b[39m mask[mask_idxs] = \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, rhs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rhs) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m + (-rhs)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rhs) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     86\u001b[39m     rhs = rhs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rhs, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(rhs)\n\u001b[32m     87\u001b[39m     out = Tensor(\u001b[38;5;28mself\u001b[39m.data*rhs.data, \u001b[38;5;28mself\u001b[39m.requires_grad \u001b[38;5;129;01mor\u001b[39;00m rhs.requires_grad, (\u001b[38;5;28mself\u001b[39m, rhs), \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7299b4aa20f0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7299b4adbb60, execution_count=20 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7299b4adbad0, raw_cell=\"train_nn(40)\n",
      "train_nn(40, p=0.05)\n",
      "train_nn(40, p=0..\" transformed_cell=\"train_nn(40)\n",
      "train_nn(40, p=0.05)\n",
      "train_nn(40, p=0..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/nik/personal_projects/dqn_from_scratch/test.ipynb#X23sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:787\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:294\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    293\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[33m\"\u001b[39m\u001b[33mpb.Record\u001b[39m\u001b[33m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m._assign(record)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:170\u001b[39m, in \u001b[36mSockClient.send_record_publish\u001b[39m\u001b[34m(self, record)\u001b[39m\n\u001b[32m    168\u001b[39m server_req.request_id = record.control.mailbox_slot\n\u001b[32m    169\u001b[39m server_req.record_publish.CopyFrom(record)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:150\u001b[39m, in \u001b[36mSockClient.send_server_request\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:147\u001b[39m, in \u001b[36mSockClient._send_message\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    145\u001b[39m header = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<BI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m), raw_size)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/DQNenv/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:126\u001b[39m, in \u001b[36mSockClient._sendall_with_error_handle\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    124\u001b[39m start_time = time.monotonic()\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     sent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sent == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "train_nn(40)\n",
    "train_nn(40, p=0.05)\n",
    "train_nn(40, p=0.1)\n",
    "train_nn(40, p=0.15)\n",
    "train_nn(40, p=0.2)\n",
    "train_nn(40, p=0.25)\n",
    "train_nn(40, p=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
