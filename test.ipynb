{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9225817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple, Self, Iterable\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6c182ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular constants\n",
    "RNG = np.random.default_rng()\n",
    "DTYPE = 'float64' \n",
    "\n",
    "# testing constants\n",
    "if DTYPE=='float64':\n",
    "    EPS, ATOL, RTOL = 1e-6, 1e-5, 1e-3\n",
    "else:\n",
    "    EPS, ATOL, RTOL = 1e-4, 1e-4, 1e-2\n",
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO \n",
    "- loss functions, implement and check correctness (CE/+softmax, mse)\n",
    "- implement adam \n",
    "- make constants/ rngs be in a seperate file\n",
    "- add in requires grad functionality (enable grad, context manager etc)\n",
    "- add in more layers convolutions, softmax, dropout, batch norm (possibly)\n",
    "- todo implement dataloaders \n",
    "- add logging, WandB and also terminal logging\n",
    "- add in auto grad visulisation\n",
    "'''\n",
    "\n",
    "class Tensor():\n",
    "    def __init__(self, data, requires_grad=False, children=(), op=''):\n",
    "        self.data: np.ndarray = np.array(data, dtype=DTYPE)\n",
    "        self.grad = np.zeros_like(data, dtype=DTYPE)\n",
    "        self.requires_grad = requires_grad\n",
    "        self._prev = set(children)\n",
    "        self._backward = lambda : None\n",
    "        self._op = op\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int]:\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int: \n",
    "        return self.data.size\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = np.zeros_like(self.data, dtype=DTYPE)\n",
    "\n",
    "    def item(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "    def _unbroadcast(self, grad: np.ndarray) -> Self:\n",
    "        dims_to_remove = tuple(i for i in range(len(grad.shape) - len(self.shape))) \n",
    "        # remove prepended padding dimensions\n",
    "        grad = np.sum(grad, axis=dims_to_remove, keepdims=False) \n",
    "        dims_to_reduce = tuple(i for i, (d1,d2) in enumerate(zip(grad.shape, self.shape)) if d1!=d2)\n",
    "        # reduce broadcasted dimensions\n",
    "        return np.sum(grad, axis=dims_to_reduce, keepdims=True)\n",
    "\n",
    "    # need to build topo graph and then go through it and call backwards on each of the tensors\n",
    "    def backward(self) -> None:\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # do DFS on un-visited nodes, add node to topo-when all its children have been visited\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            \n",
    "    def __add__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data + rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '+')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self) -> Self:\n",
    "        out = Tensor(-self.data, self.requires_grad, (self,), 'neg')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, rhs) -> Self:\n",
    "        return self + (-rhs)\n",
    "\n",
    "    def __mul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data*rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), f'*')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * rhs.data)\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, rhs) -> Self:\n",
    "        return self * (rhs**-1)\n",
    "    \n",
    "    # TODO add check for rhs, if epxponent if negative the gradient is undefined\n",
    "    def __pow__(self, rhs) -> Self: \n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        lhs_is_neg = self.data < 0\n",
    "        rhs_is_frac = ~np.isclose(rhs.data % 1, 0)\n",
    "        if np.any(lhs_is_neg & rhs_is_frac):\n",
    "            raise ValueError('cannot raise negative value to a decimal power')\n",
    "        \n",
    "        out = Tensor(self.data**rhs.data, self.requires_grad or rhs.requires_grad, (self,), f'**')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(out.grad * ((rhs.data)*(self.data**(rhs.data-1))))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(out.grad * (self.data ** rhs.data) * np.log(self.data))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''data shape: (da, ..., d2, d1, n, k) rhs shape: (ob, ..., o2, o1, k, m)\n",
    "       inputs are broadcast so that they have the same shape by expanding along\n",
    "       dimensions if possible, out shape: (tc, ..., t2, t1, n, m), where ti = max(di, oi)\n",
    "       if di or oi does not exist it is treated as 1, and c = max d, a\n",
    "       if self is 1d shape is prepended with a 1, for rhs it would be appended'''\n",
    "    def __matmul__(self, rhs) -> Self:\n",
    "        rhs = rhs if isinstance(rhs, Tensor) else Tensor(rhs)\n",
    "        out = Tensor(self.data @ rhs.data, self.requires_grad or rhs.requires_grad, (self, rhs), '@')\n",
    "\n",
    "        def _backward():\n",
    "            A, B, = self.data, rhs.data\n",
    "            g = out.grad\n",
    "            # broadcast 1d arrays to be 2d \n",
    "            A2 = A.reshape(1, -1) if len(A.shape) == 1 else A\n",
    "            B2 = B.reshape(-1, 1) if len(B.shape) == 1 else B\n",
    "            # extend g to have reduced dims\n",
    "            g = np.expand_dims(g, -1) if len(B.shape) == 1 else g\n",
    "            g = np.expand_dims(g, -2) if len(A.shape) == 1 else g\n",
    "            # transpose last 2 dimensions, as matmul treats tensors as batched matricies\n",
    "            if self.requires_grad:\n",
    "                self.grad += self._unbroadcast(g @ B2.swapaxes(-2, -1))\n",
    "            if rhs.requires_grad:\n",
    "                rhs.grad += rhs._unbroadcast(A2.swapaxes(-2, -1) @ g)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Self:\n",
    "        out = Tensor((self.data > 0) * self.data, self.requires_grad, (self,), 'Relu')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # need to check inp is non-negative\n",
    "    def log(self) -> Self:\n",
    "        if np.any(self.data < 0):\n",
    "            raise ValueError('cannot log negative values')\n",
    "        out = Tensor(np.log(self.data), self.requires_grad, (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += 1 / self.data \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self) -> Self:\n",
    "        out = Tensor(np.exp(self.data), self.requires_grad, (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.exp(self.data)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None) -> Self:\n",
    "        out = Tensor(np.sum(self.data, axis=axis), self.requires_grad, (self,), 'sum')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "                self.grad += g\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self, axis=None) -> Self:\n",
    "        out = Tensor(np.mean(self.data, axis=axis), self.requires_grad, (self,), 'mean')\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                N =  self.size // out.size \n",
    "                g = np.expand_dims(out.grad, axis) if axis is not None else out.grad\n",
    "                self.grad += g / N\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rsub__(self, lhs) -> Self:\n",
    "        return self + lhs\n",
    "    \n",
    "    def __rmul__(self, lhs) -> Self:\n",
    "        return self * lhs\n",
    "    \n",
    "    def __rtruediv__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs / self\n",
    "    \n",
    "    def __rpow__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs ** self\n",
    "    \n",
    "    def __rmatmul__(self, lhs) -> Self:\n",
    "        try:\n",
    "            lhs = Tensor(lhs)\n",
    "        except TypeError:\n",
    "            return NotImplementedError\n",
    "        return lhs @ self\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape: tuple, bounds = (0,1), requires_grad=False) -> Self:\n",
    "        lower, upper = bounds\n",
    "        data = RNG.random(shape, dtype=DTYPE)*(upper-lower) + lower\n",
    "        return cls(data, requires_grad=requires_grad)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'tensor shape: {self.shape}, op:{self._op}'        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f977fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data, requires_grad=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def kaiming(cls, fan_in, shape):\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        weights = RNG.standard_normal(shape, dtype=DTYPE)*std\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        return cls(np.zeros(shape, dtype=DTYPE))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'parameter shape: {self.shape}, size: {self.size}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    \n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        return self.forward(input)\n",
    "    \n",
    "    @property\n",
    "    def modules(self) -> list[Self]:\n",
    "        modules: list[Self] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, Module):\n",
    "                modules.append(value)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                for v in value.values():\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "\n",
    "            elif isinstance(value, Iterable) and not isinstance(value, (str, bytes)):\n",
    "                for v in value:\n",
    "                    if isinstance(v, Module):\n",
    "                        modules.append(v)\n",
    "                    \n",
    "        return modules\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> list[Parameter]:\n",
    "        immediate_params = [attr for attr in self.__dict__.values() \n",
    "                                    if isinstance(attr, Parameter)]\n",
    "        modules_params = [param for module in self.modules \n",
    "                                    for param in module.params]\n",
    "        return immediate_params + modules_params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Affine(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.A = Parameter.kaiming(in_dim, (in_dim, out_dim))\n",
    "        self.b = Parameter.zeros((out_dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # x: (B, in), A : (in, out), B: out\n",
    "        return (x @ self.A) + self.b\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.relu()\n",
    "    \n",
    "class SoftMax(Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        x = x.exp()\n",
    "        norm_c = x.sum()\n",
    "        return x / norm_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a67cdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxCrossEntropy():\n",
    "\n",
    "    def __call__(z: Tensor, y) -> Tensor:\n",
    "        '''logits z, shape (B, C), true lables y, shape (B, C)'''\n",
    "        loss = ((z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "        return loss\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params: list[Parameter], lr: float):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.data += -self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d3a259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimiser, loss, train_loader, test_loader, logger, wandb_run = None):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.loss = loss\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epoch = 1\n",
    "        self.logger = logger\n",
    "        self.wandb_run = wandb_run\n",
    "\n",
    "    def train_epoch():\n",
    "        pass\n",
    "\n",
    "    def validate():\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def log_metrics():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dd501775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (50, 100), size: 5000,\n",
       " parameter shape: (100,), size: 100,\n",
       " parameter shape: (100, 200), size: 20000,\n",
       " parameter shape: (200,), size: 200,\n",
       " parameter shape: (200, 10), size: 2000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 100), Relu(), Affine(100, 200), Relu(), Affine(200, 10)])\n",
    "feedforward.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "529f4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' auto-grad testing suite\n",
    "    TODO:\n",
    "    - test all of the auto-grad primatives, \n",
    "    - test using central differences\n",
    "    - test by modifying each parameter individually i.e. only do scalar pertubations\n",
    "'''\n",
    "\n",
    "def compute_central_diff_error(test_fn, test_input, \n",
    "            other_inputs, eps, perturbed_idx, tols):\n",
    "    '''verify auto-grad of funciton f: R^n -> R'''\n",
    "    atol, rtol = tols\n",
    "\n",
    "    # rescale epsilon and convert to tensor\n",
    "    perturbed_val = test_input.data[perturbed_idx]\n",
    "    eps = eps * (1 + abs(perturbed_val))\n",
    "    pertubation_tensor = np.zeros_like(test_input.data, dtype=DTYPE)\n",
    "    pertubation_tensor[perturbed_idx] += eps \n",
    "    pertubation_tensor = Tensor(pertubation_tensor)\n",
    "\n",
    "    # Compute grad\n",
    "    for tensor in [test_input, *other_inputs]:\n",
    "        tensor.zero_grad()\n",
    "    clean_out = test_fn(test_input, other_inputs)\n",
    "    clean_out.backward()\n",
    "    auto_grad = test_input.grad[perturbed_idx]\n",
    "\n",
    "    # Compute central diff Grad approximaiton\n",
    "    test_forward = test_input + pertubation_tensor\n",
    "    forward_out = test_fn(test_forward, other_inputs).item()\n",
    "    test_back = test_input - pertubation_tensor\n",
    "    back_out = test_fn(test_back, other_inputs).item()\n",
    "    approx_grad = (forward_out - back_out) / (2*eps)\n",
    "\n",
    "\n",
    "    abs_err = abs(approx_grad - auto_grad)\n",
    "    rel_err = abs_err / (abs(auto_grad) + atol)\n",
    "    is_close = abs_err <= atol + rtol*abs(auto_grad)\n",
    "\n",
    "    return is_close, abs_err, rel_err, clean_out.item(), forward_out, back_out\n",
    "\n",
    "# need to generate inputs, compute cd err and output/format test result, to log file maybe?\n",
    "def test_fn_random_inputs(test_fn, test_shape, other_shapes=[], input_bounds=(-5, 5),\n",
    "                          num_samples=K, eps=EPS, tols=(ATOL, RTOL)):\n",
    "    \n",
    "    test_input = Tensor.random(test_shape, input_bounds, requires_grad=True)\n",
    "    other_inputs = [Tensor.random(shape, input_bounds) for shape in other_shapes]\n",
    "\n",
    "    num_samples = min(test_input.size, num_samples)\n",
    "    pertubation_nums = RNG.choice(test_input.size, size=num_samples, replace=False)\n",
    "    pretubation_idxs = np.unravel_index(pertubation_nums, test_shape)\n",
    "\n",
    "    all_close = True\n",
    "    failed = 0\n",
    "    # log = inspect.getsource(test_fn) + '\\n' \n",
    "    log = ''\n",
    "    log += f'test input \\n {test_input.data} \\nother inputs \\n'\n",
    "    for other_input in other_inputs:\n",
    "        log += f' {other_input.data} \\n'\n",
    "    for sample_i in range(num_samples):\n",
    "        perturbed_idx = tuple(int(pert_dim[sample_i]) for pert_dim in pretubation_idxs)\n",
    "        is_close, abs_err, rel_err, clean_out, forward_out, back_out = compute_central_diff_error(\n",
    "                                        test_fn, test_input, other_inputs, eps, perturbed_idx, tols)\n",
    "        log += f'test {'passed' if is_close else 'failed'}: abs err = {abs_err:.4f}, rel err = {rel_err:.4f}, perturbed idx = {perturbed_idx} \\n'\n",
    "        log += f'clean_out: {clean_out} forward_out: {forward_out} back_out: {back_out} \\n'\n",
    "        if not is_close:\n",
    "            all_close = False\n",
    "            failed += 1\n",
    "            # some logic for logging the failed case\n",
    "\n",
    "    return all_close, log\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fdca5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ufuncs = {'add' : lambda test_inp, other_inps: (test_inp+other_inps[0]).sum(),\n",
    "              'radd': lambda test_inp, other_inps: (other_inps[0]+test_inp).sum(),\n",
    "              'sub' : lambda test_inp, other_inps: (test_inp-other_inps[0]).sum(),\n",
    "              'rsub': lambda test_inp, other_inps: (other_inps[0]-test_inp).sum(),\n",
    "              'mul' : lambda test_inp, other_inps: (test_inp*other_inps[0]).sum(),\n",
    "              'rmul': lambda test_inp, other_inps: (other_inps[0]*test_inp).sum(),\n",
    "              'pow' : lambda test_inp, other_inps: (test_inp**other_inps[0]).sum(),\n",
    "              'rpow': lambda test_inp, other_inps: (other_inps[0]**test_inp).sum(),\n",
    "              'truediv' : lambda test_inp, other_inps: (test_inp/other_inps[0]).sum(),\n",
    "              'rtruediv': lambda test_inp, other_inps: (other_inps[0]/test_inp).sum(),}\n",
    "\n",
    "matmul_fns = {'matmul': lambda test_inp, other_inps: (test_inp@other_inps[0]).sum(),\n",
    "              'rmatmul': lambda test_inp, other_inps: (other_inps[0]@test_inp).sum(),}\n",
    "\n",
    "unary_ufunc = {'relu': lambda test_inp, other_inps: (test_inp.relu()).sum(),\n",
    "            'log': lambda test_inp, other_inps: (test_inp.log()).sum(),\n",
    "            'exp': lambda test_inp, other_inps: (test_inp.exp()).sum(),\n",
    "            'sum': lambda test_inp, other_inps: test_inp.sum(),\n",
    "            'mean': lambda test_inp, other_inps: test_inp.mean(),}\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ce2720b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: relu passed\n",
      "function: log passed\n",
      "function: exp passed\n",
      "function: sum passed\n",
      "function: mean passed\n",
      "function: matmul passed\n",
      "function: rmatmul passed\n",
      "function: add passed\n",
      "function: radd passed\n",
      "function: sub passed\n",
      "function: rsub passed\n",
      "function: mul passed\n",
      "function: rmul passed\n",
      "function: pow passed\n",
      "function: rpow passed\n",
      "function: truediv passed\n",
      "function: rtruediv passed\n"
     ]
    }
   ],
   "source": [
    "for func_name, test_fn in unary_ufunc.items():\n",
    "    test_shape, other_shapes = (2, 3), [(3,2)]\n",
    "    input_bounds = (1, 10) if func_name == 'log' else (-5, 5)\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)\n",
    "\n",
    "for func_name, test_fn in matmul_fns.items():\n",
    "    test_shape = (2, 3) if func_name == 'matmul' else (3, 2)\n",
    "    other_shapes = [(3, 2)] if func_name == 'matmul' else [(2, 3)]\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)\n",
    "\n",
    "for func_name, test_fn in bin_ufuncs.items():\n",
    "    test_shape, other_shapes = (2, 3), [(2,3)]\n",
    "    input_bounds = (1, 5) if (func_name == 'pow' or func_name == 'rpow') else (-5, 5)\n",
    "    all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes, input_bounds=input_bounds)\n",
    "    print(f'function: {func_name} {'passed' if all_close else 'failed'}')\n",
    "    if not all_close:\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "32ab8d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parameter shape: (50, 1000), size: 50000,\n",
       " parameter shape: (1000,), size: 1000,\n",
       " parameter shape: (1000, 2000), size: 2000000,\n",
       " parameter shape: (2000,), size: 2000,\n",
       " parameter shape: (2000, 10), size: 20000,\n",
       " parameter shape: (10,), size: 10]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = Sequential([Affine(50, 1000), Relu(), Affine(1000, 2000), Relu(), Affine(2000, 10)])\n",
    "feedforward.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4cdcb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "test input \n",
      " [[ 3.75026808  3.79064791  3.35255986 ...  2.32166393  0.74594803\n",
      "  -0.46479737]\n",
      " [-4.82424674  3.04718977 -0.47803226 ... -0.06946837 -4.47902869\n",
      "  -4.57079004]\n",
      " [-1.10095121  4.44193048 -2.13587614 ...  2.43866997 -1.71638962\n",
      "  -1.90082661]\n",
      " ...\n",
      " [-1.16603229 -1.89337666 -3.99321261 ...  1.50069455 -3.46158155\n",
      "   0.05832781]\n",
      " [ 0.54550074  0.25045465 -4.41086695 ...  4.59098623  2.6388176\n",
      "   4.59158431]\n",
      " [-1.03219612  0.01671006  1.78371654 ... -0.51904045 -4.09778641\n",
      "  -4.67551704]] \n",
      "other inputs \n",
      " [3.17973245] \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (250, 23) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592911575 back_out: 5613.615596890448 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (734, 37) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592760412 back_out: 5613.615597041609 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (176, 8) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615594970762 back_out: 5613.61559483126 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (735, 15) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615599949473 back_out: 5613.615589852548 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (747, 35) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615591237306 back_out: 5613.615598564715 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (170, 23) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592843867 back_out: 5613.615596958155 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (423, 36) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615596074319 back_out: 5613.615593727702 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (770, 44) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615595716024 back_out: 5613.615594085998 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (461, 23) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615595019368 back_out: 5613.615594782653 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (285, 0) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615595965058 back_out: 5613.615593836965 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (404, 16) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615601386643 back_out: 5613.615588415379 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (196, 4) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615594747067 back_out: 5613.615595054955 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (875, 31) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615594201377 back_out: 5613.615595600645 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (577, 32) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592635824 back_out: 5613.6155971661965 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (793, 36) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.6155942565 back_out: 5613.6155955455215 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (323, 45) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.6155971792405 back_out: 5613.615592622782 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (184, 9) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615595020641 back_out: 5613.615594781381 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (474, 0) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615598431578 back_out: 5613.615591370443 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (909, 8) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592254034 back_out: 5613.615597547989 \n",
      "test passed: abs err = 0.0000, rel err = 0.0000, perturbed idx = (806, 31) \n",
      "clean_out: 5613.615594901011 forward_out: 5613.615592671706 back_out: 5613.615597130315 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_shape, other_shapes = (1000, 50), (1,)\n",
    "test_fn = lambda test, other: feedforward(test).sum()\n",
    "all_close, log = test_fn_random_inputs(test_fn, test_shape, other_shapes)\n",
    "\n",
    "print(all_close)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2dc5cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[[5.00000000e-01 2.68811714e+43]\n",
      " [2.68811714e+43 5.00000000e-01]]\n",
      "[[-50.  50.]\n",
      " [ 50. -50.]]\n"
     ]
    }
   ],
   "source": [
    "z = Tensor(np.array([[-100, 100], [100, -100]]), requires_grad=True)\n",
    "y = Tensor(np.array([[1.0, 0], [0.0, 1.0]]), requires_grad=True)\n",
    "crossentropy = SoftMaxCrossEntropy()\n",
    "b = ((z * y).sum(axis=-1) + ((z.exp()).sum(axis=-1)).log()).mean()\n",
    "# b = crossentropy(a, c)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(z.grad)\n",
    "print(y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
